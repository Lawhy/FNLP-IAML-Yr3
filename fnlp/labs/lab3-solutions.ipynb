{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNLP: Lab Session 3\n",
    "\n",
    "### Hidden Markov Models - Construction and Use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages used for this lab\n",
    "\n",
    "import nltk\n",
    "\n",
    "# import brown corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# module for training a Hidden Markov Model and tagging sequences\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger\n",
    "\n",
    "# module for computing a Conditional Frequency Distribution\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "# module for computing a Conditional Probability Distribution\n",
    "from nltk.probability import ConditionalProbDist\n",
    "\n",
    "# module for computing a probability distribution with the Maximum Likelihood Estimate\n",
    "from nltk.probability import MLEProbDist\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Corpora tagged with part-of-speech information\n",
    "\n",
    "NLTK provides corpora annotated with part-of-speech (POS) information and\n",
    "some tools to access this information. The Penn Treebank tagset is commonly\n",
    "used for annotating English sentences. We can inspect this tagset in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus provided with NLTK is also tagged with POS information,\n",
    "although the tagset is slightly different than the Penn Treebank tagset. Information about the Brown corpus tagset can be found here:\n",
    "http://www.scs.leeds.ac.uk/ccalas/tagsets/brown.html\n",
    "\n",
    "We can retrieve the tagged sentences in the Brown corpus by calling the `tagged_sents()`\n",
    "function and looking at an annotated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tagged with Penn Treebank POS labels:\n",
      "[('The', 'AT'), ('jury', 'NN'), ('praised', 'VBD'), ('the', 'AT'), ('administration', 'NN'), ('and', 'CC'), ('operation', 'NN'), ('of', 'IN'), ('the', 'AT'), ('Atlanta', 'NP-TL'), ('Police', 'NNS-TL'), ('Department', 'NN-TL'), (',', ','), ('the', 'AT'), ('Fulton', 'NP-TL'), ('Tax', 'NN-TL'), (\"Commissioner's\", 'NN$-TL'), ('Office', 'NN-TL'), (',', ','), ('the', 'AT'), ('Bellwood', 'NP'), ('and', 'CC'), ('Alpharetta', 'NP'), ('prison', 'NN'), ('farms', 'NNS'), (',', ','), ('Grady', 'NP-TL'), ('Hospital', 'NN-TL'), ('and', 'CC'), ('the', 'AT'), ('Fulton', 'NP-TL'), ('Health', 'NN-TL'), ('Department', 'NN-TL'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = brown.tagged_sents(categories='news')\n",
    "print('Sentence tagged with Penn Treebank POS labels:')\n",
    "print(tagged_sentences[29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to use a coarser label set in order to avoid data sparsity\n",
    "or to allow a mapping between the POS labels for different languages. The Universal tagset was designed to be applicable for all languages:\n",
    "\n",
    "https://code.google.com/p/universalpostags/.\n",
    "\n",
    "There are mappings between the POS tagset of several languages and the Universal tagset. We can access the Universal tags for the Brown corpus sentences\n",
    "by changing the tagset argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tagged with Universal POS:\n",
      "[('The', 'DET'), ('jury', 'NOUN'), ('praised', 'VERB'), ('the', 'DET'), ('administration', 'NOUN'), ('and', 'CONJ'), ('operation', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('Atlanta', 'NOUN'), ('Police', 'NOUN'), ('Department', 'NOUN'), (',', '.'), ('the', 'DET'), ('Fulton', 'NOUN'), ('Tax', 'NOUN'), (\"Commissioner's\", 'NOUN'), ('Office', 'NOUN'), (',', '.'), ('the', 'DET'), ('Bellwood', 'NOUN'), ('and', 'CONJ'), ('Alpharetta', 'NOUN'), ('prison', 'NOUN'), ('farms', 'NOUN'), (',', '.'), ('Grady', 'NOUN'), ('Hospital', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('Fulton', 'NOUN'), ('Health', 'NOUN'), ('Department', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences_universal = brown.tagged_sents(categories='news', tagset='universal')\n",
    "print('Sentence tagged with Universal POS:')\n",
    "print(tagged_sentences_universal[29])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1:\n",
    "\n",
    "In this exercise we will compute a Frequency Distribution over tags that appear\n",
    "in the Brown corpus. The template of the function that you have to implement\n",
    "takes two parameters: one is the category of the text and the other is the tagset\n",
    "name. You are given the code to retrieve the list of (word, tag) tuples from the\n",
    "brown corpus corresponding to the given category and tagset.\n",
    "\n",
    "1. Convert the list of word+tag pairs to a list of tags\n",
    "2. Using the list of tags to compute a frequency distribution over the tags, useing `FreqDist()`\n",
    "3. Compute the total number of tags in the Frequency Distribution\n",
    "4. Retrieve the top 10 most frequent tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# EXERCISE 1 #################\n",
    "\n",
    "# Solution for exercise 1\n",
    "\n",
    "def ex1(genre, tagset):\n",
    "    \"\"\"Compute a Frequency distribution of the POS tags in a genre of the tagged Brown corpus\n",
    "    :param genre: A Brown corpus genre\n",
    "    :type genre: str or iterable(str) or None\n",
    "    :param tagset: A Brown tagset name\n",
    "    :type tagset: str or None (defaults to 'brown')\n",
    "    :return: number of tag types, top 10 tags\n",
    "    :rtype: tuple(int,list(tuple(str,int))\"\"\"\n",
    "    # get the tagged words from the corpus\n",
    "    tagged_words = brown.tagged_words(categories=genre, tagset=tagset)\n",
    "\n",
    "    # TODO: convert tagged words to a list of tags\n",
    "    tags = ( tp[1] for tp in tagged_words)\n",
    "  \n",
    "    # TODO: using the above list compute a Frequency Distribution\n",
    "    # hint: use nltk.FreqDist()\n",
    "    tagsFDist = nltk.FreqDist(tags)\n",
    "  \n",
    "    # TODO: retrieve the number of tag types in the tagset\n",
    "    number_of_tags = tagsFDist.B()\n",
    "  \n",
    "    #TODO: retrieve the top 10 most frequent tags\n",
    "    top_tags = tagsFDist.most_common(10)\n",
    "\n",
    "    return number_of_tags,top_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag FreqDist for news:\n",
      "(218, [('NN', 13162), ('IN', 10616), ('AT', 8893), ('NP', 6866), (',', 5133), ('NNS', 5066), ('.', 4452), ('JJ', 4392), ('CC', 2664), ('VBD', 2524)])\n",
      "Tag FreqDist for science_fiction:\n",
      "(127, [('NN', 1541), ('IN', 1176), ('.', 1077), ('AT', 1040), (',', 791), ('JJ', 723), ('NNS', 532), ('VBD', 531), ('RB', 522), ('VB', 495)])\n",
      "Tag FreqDist for news with Universal tagset:\n",
      "(12, [('NOUN', 30654), ('VERB', 14399), ('ADP', 12355), ('.', 11928), ('DET', 11389), ('ADJ', 6706), ('ADV', 3349), ('CONJ', 2717), ('PRON', 2535), ('PRT', 2264)])\n",
      "Tag FreqDist for science_fiction with Universal tagset:\n",
      "(12, [('NOUN', 2747), ('VERB', 2579), ('.', 2428), ('DET', 1582), ('ADP', 1451), ('PRON', 934), ('ADJ', 929), ('ADV', 828), ('PRT', 483), ('CONJ', 416)])\n"
     ]
    }
   ],
   "source": [
    "# Test your code for exercise 1\n",
    "\n",
    "def test_ex1():\n",
    "    print('Tag FreqDist for news:')\n",
    "    print(ex1('news', None))\n",
    "\n",
    "    print('Tag FreqDist for science_fiction:')\n",
    "    print(ex1('science_fiction', None))\n",
    "\n",
    "    # Do the same thing for a different tagset: Universal.  Observe the differences.\n",
    "\n",
    "    print('Tag FreqDist for news with Universal tagset:')\n",
    "    print(ex1('news', 'universal'))\n",
    "\n",
    "    print('Tag FreqDist for science_fiction with Universal tagset:')\n",
    "    print(ex1('science_fiction', 'universal'))\n",
    "\n",
    "# Let's look at the top tags for different genre and tagsets. Observe differences\n",
    "test_ex1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating an HMM Tagger\n",
    "\n",
    "NLTK provides a module for training a Hidden Markov Model for sequence tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HiddenMarkovModelTagger in module nltk.tag.hmm:\n",
      "\n",
      "class HiddenMarkovModelTagger(nltk.tag.api.TaggerI)\n",
      " |  HiddenMarkovModelTagger(symbols, states, transitions, outputs, priors, transform=<function _identity at 0x7fb707e94400>)\n",
      " |  \n",
      " |  Hidden Markov model class, a generative model for labelling sequence data.\n",
      " |  These models define the joint probability of a sequence of symbols and\n",
      " |  their labels (state transitions) as the product of the starting state\n",
      " |  probability, the probability of each state transition, and the probability\n",
      " |  of each observation being generated from each state. This is described in\n",
      " |  more detail in the module documentation.\n",
      " |  \n",
      " |  This implementation is based on the HMM description in Chapter 8, Huang,\n",
      " |  Acero and Hon, Spoken Language Processing and includes an extension for\n",
      " |  training shallow HMM parsers or specialized HMMs as in Molina et.\n",
      " |  al, 2002.  A specialized HMM modifies training data by applying a\n",
      " |  specialization function to create a new training set that is more\n",
      " |  appropriate for sequential tagging with an HMM.  A typical use case is\n",
      " |  chunking.\n",
      " |  \n",
      " |  :param symbols: the set of output symbols (alphabet)\n",
      " |  :type symbols: seq of any\n",
      " |  :param states: a set of states representing state space\n",
      " |  :type states: seq of any\n",
      " |  :param transitions: transition probabilities; Pr(s_i | s_j) is the\n",
      " |      probability of transition from state i given the model is in\n",
      " |      state_j\n",
      " |  :type transitions: ConditionalProbDistI\n",
      " |  :param outputs: output probabilities; Pr(o_k | s_i) is the probability\n",
      " |      of emitting symbol k when entering state i\n",
      " |  :type outputs: ConditionalProbDistI\n",
      " |  :param priors: initial state distribution; Pr(s_i) is the probability\n",
      " |      of starting in state i\n",
      " |  :type priors: ProbDistI\n",
      " |  :param transform: an optional function for transforming training\n",
      " |      instances, defaults to the identity function.\n",
      " |  :type transform: callable\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      HiddenMarkovModelTagger\n",
      " |      nltk.tag.api.TaggerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, symbols, states, transitions, outputs, priors, transform=<function _identity at 0x7fb707e94400>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  best_path(self, unlabeled_sequence)\n",
      " |      Returns the state sequence of the optimal (most probable) path through\n",
      " |      the HMM. Uses the Viterbi algorithm to calculate this part by dynamic\n",
      " |      programming.\n",
      " |      \n",
      " |      :return: the state sequence\n",
      " |      :rtype: sequence of any\n",
      " |      :param unlabeled_sequence: the sequence of unlabelled symbols\n",
      " |      :type unlabeled_sequence: list\n",
      " |  \n",
      " |  best_path_simple(self, unlabeled_sequence)\n",
      " |      Returns the state sequence of the optimal (most probable) path through\n",
      " |      the HMM. Uses the Viterbi algorithm to calculate this part by dynamic\n",
      " |      programming.  This uses a simple, direct method, and is included for\n",
      " |      teaching purposes.\n",
      " |      \n",
      " |      :return: the state sequence\n",
      " |      :rtype: sequence of any\n",
      " |      :param unlabeled_sequence: the sequence of unlabeled symbols\n",
      " |      :type unlabeled_sequence: list\n",
      " |  \n",
      " |  entropy(self, unlabeled_sequence)\n",
      " |      Returns the entropy over labellings of the given sequence. This is\n",
      " |      given by::\n",
      " |      \n",
      " |          H(O) = - sum_S Pr(S | O) log Pr(S | O)\n",
      " |      \n",
      " |      where the summation ranges over all state sequences, S. Let\n",
      " |      *Z = Pr(O) = sum_S Pr(S, O)}* where the summation ranges over all state\n",
      " |      sequences and O is the observation sequence. As such the entropy can\n",
      " |      be re-expressed as::\n",
      " |      \n",
      " |          H = - sum_S Pr(S | O) log [ Pr(S, O) / Z ]\n",
      " |          = log Z - sum_S Pr(S | O) log Pr(S, 0)\n",
      " |          = log Z - sum_S Pr(S | O) [ log Pr(S_0) + sum_t Pr(S_t | S_{t-1}) + sum_t Pr(O_t | S_t) ]\n",
      " |      \n",
      " |      The order of summation for the log terms can be flipped, allowing\n",
      " |      dynamic programming to be used to calculate the entropy. Specifically,\n",
      " |      we use the forward and backward probabilities (alpha, beta) giving::\n",
      " |      \n",
      " |          H = log Z - sum_s0 alpha_0(s0) beta_0(s0) / Z * log Pr(s0)\n",
      " |          + sum_t,si,sj alpha_t(si) Pr(sj | si) Pr(O_t+1 | sj) beta_t(sj) / Z * log Pr(sj | si)\n",
      " |          + sum_t,st alpha_t(st) beta_t(st) / Z * log Pr(O_t | st)\n",
      " |      \n",
      " |      This simply uses alpha and beta to find the probabilities of partial\n",
      " |      sequences, constrained to include the given state(s) at some point in\n",
      " |      time.\n",
      " |  \n",
      " |  log_probability(self, sequence)\n",
      " |      Returns the log-probability of the given symbol sequence. If the\n",
      " |      sequence is labelled, then returns the joint log-probability of the\n",
      " |      symbol, state sequence. Otherwise, uses the forward algorithm to find\n",
      " |      the log-probability over all label sequences.\n",
      " |      \n",
      " |      :return: the log-probability of the sequence\n",
      " |      :rtype: float\n",
      " |      :param sequence: the sequence of symbols which must contain the TEXT\n",
      " |          property, and optionally the TAG property\n",
      " |      :type sequence:  Token\n",
      " |  \n",
      " |  point_entropy(self, unlabeled_sequence)\n",
      " |      Returns the pointwise entropy over the possible states at each\n",
      " |      position in the chain, given the observation sequence.\n",
      " |  \n",
      " |  probability(self, sequence)\n",
      " |      Returns the probability of the given symbol sequence. If the sequence\n",
      " |      is labelled, then returns the joint probability of the symbol, state\n",
      " |      sequence. Otherwise, uses the forward algorithm to find the\n",
      " |      probability over all label sequences.\n",
      " |      \n",
      " |      :return: the probability of the sequence\n",
      " |      :rtype: float\n",
      " |      :param sequence: the sequence of symbols which must contain the TEXT\n",
      " |          property, and optionally the TAG property\n",
      " |      :type sequence:  Token\n",
      " |  \n",
      " |  random_sample(self, rng, length)\n",
      " |      Randomly sample the HMM to generate a sentence of a given length. This\n",
      " |      samples the prior distribution then the observation distribution and\n",
      " |      transition distribution for each subsequent observation and state.\n",
      " |      This will mostly generate unintelligible garbage, but can provide some\n",
      " |      amusement.\n",
      " |      \n",
      " |      :return:        the randomly created state/observation sequence,\n",
      " |                      generated according to the HMM's probability\n",
      " |                      distributions. The SUBTOKENS have TEXT and TAG\n",
      " |                      properties containing the observation and state\n",
      " |                      respectively.\n",
      " |      :rtype:         list\n",
      " |      :param rng:     random number generator\n",
      " |      :type rng:      Random (or any object with a random() method)\n",
      " |      :param length:  desired output length\n",
      " |      :type length:   int\n",
      " |  \n",
      " |  reset_cache(self)\n",
      " |  \n",
      " |  tag(self, unlabeled_sequence)\n",
      " |      Tags the sequence with the highest probability state sequence. This\n",
      " |      uses the best_path method to find the Viterbi path.\n",
      " |      \n",
      " |      :return: a labelled sequence of symbols\n",
      " |      :rtype: list\n",
      " |      :param unlabeled_sequence: the sequence of unlabeled symbols\n",
      " |      :type unlabeled_sequence: list\n",
      " |  \n",
      " |  test(self, test_sequence, verbose=False, **kwargs)\n",
      " |      Tests the HiddenMarkovModelTagger instance.\n",
      " |      \n",
      " |      :param test_sequence: a sequence of labeled test instances\n",
      " |      :type test_sequence: list(list)\n",
      " |      :param verbose: boolean flag indicating whether training should be\n",
      " |          verbose or include printed output\n",
      " |      :type verbose: bool\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  train(labeled_sequence, test_sequence=None, unlabeled_sequence=None, **kwargs) from abc.ABCMeta\n",
      " |      Train a new HiddenMarkovModelTagger using the given labeled and\n",
      " |      unlabeled training instances. Testing will be performed if test\n",
      " |      instances are provided.\n",
      " |      \n",
      " |      :return: a hidden markov model tagger\n",
      " |      :rtype: HiddenMarkovModelTagger\n",
      " |      :param labeled_sequence: a sequence of labeled training instances,\n",
      " |          i.e. a list of sentences represented as tuples\n",
      " |      :type labeled_sequence: list(list)\n",
      " |      :param test_sequence: a sequence of labeled test instances\n",
      " |      :type test_sequence: list(list)\n",
      " |      :param unlabeled_sequence: a sequence of unlabeled training instances,\n",
      " |          i.e. a list of sentences represented as words\n",
      " |      :type unlabeled_sequence: list(list)\n",
      " |      :param transform: an optional function for transforming training\n",
      " |          instances, defaults to the identity function, see ``transform()``\n",
      " |      :type transform: function\n",
      " |      :param estimator: an optional function or class that maps a\n",
      " |          condition's frequency distribution to its probability\n",
      " |          distribution, defaults to a Lidstone distribution with gamma = 0.1\n",
      " |      :type estimator: class or function\n",
      " |      :param verbose: boolean flag indicating whether training should be\n",
      " |          verbose or include printed output\n",
      " |      :type verbose: bool\n",
      " |      :param max_iterations: number of Baum-Welch interations to perform\n",
      " |      :type max_iterations: int\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  evaluate(self, gold)\n",
      " |      Score the accuracy of the tagger against the gold standard.\n",
      " |      Strip the tags from the gold standard text, retag it using\n",
      " |      the tagger, then compute the accuracy score.\n",
      " |      \n",
      " |      :type gold: list(list(tuple(str, str)))\n",
      " |      :param gold: The list of tagged sentences to score the tagger on.\n",
      " |      :rtype: float\n",
      " |  \n",
      " |  tag_sents(self, sentences)\n",
      " |      Apply ``self.tag()`` to each element of *sentences*.  I.e.:\n",
      " |      \n",
      " |          return [self.tag(sent) for sent in sentences]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.tag.hmm.HiddenMarkovModelTagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train the HMM for POS tagging given a labelled dataset. In Section 1\n",
    "of this lab we learned how to access the labelled sentences of the Brown corpus.\n",
    "We will use this dataset to study the effect of the size of the training corpus on\n",
    "the accuracy of the tagger.\n",
    "\n",
    "#### Exercise 2:\n",
    "\n",
    "In this exercise we will train a HMM tagger on a training set and evaluate it\n",
    "on a test set. The template of the function that you have to implement takes\n",
    "two parameters: a sentence to be tagged and the size of the training corpus in\n",
    "number of sentences. You are given the code that creates the training and test\n",
    "datasets from the tagged sentences in the Brown corpus.\n",
    "\n",
    "1. Train a Hidden Markov Model tagger on the training dataset. Refer to `help(nltk.tag.hmm.HiddenMarkovModelTagger.train)` if necessary.\n",
    "2. Use the trained model to tag the sentence\n",
    "3. Use the trained model to evaluate the tagger on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# EXERCISE 2 #################\n",
    "\n",
    "# Solution for exercise 2\n",
    "\n",
    "def ex2(sentence, size):\n",
    "    \"\"\" Create an HMM tagger from the Brown news corpus, and test it by tagging a sample sentence\n",
    "    \n",
    "    :param sentence: An untagged sentence as an example\n",
    "    :type sentence: list(str)\n",
    "    :param size: Number of sentences to train on (be sure to leave room for the test data)\n",
    "    :type size: int\n",
    "    :return: The tagger, the sample sentence with tags, entropy of model wrt 100 test sentences\n",
    "    :rtype: tuple(nltk.tag.hmm.HiddenMarkovModelTagger, list(tuple(str,str)), float)\"\"\"\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "    # set up the training data\n",
    "    train_data = tagged_sentences[-size:]\n",
    "\n",
    "    # set up the test data\n",
    "    test_data = tagged_sentences[:100]\n",
    "    \n",
    "    # Hint: use help on HiddenMarkovModelTagger to find out how to train, tag and evaluate an HMM tagger\n",
    "\n",
    "    # TODO: train a HiddenMarkovModelTagger, using the train() method\n",
    "    tagger = HiddenMarkovModelTagger.train(train_data)\n",
    "\n",
    "    # TODO: using the hmm tagger tag the sentence\n",
    "    hmm_tagged_sentence = tagger.tag(sentence)\n",
    "    \n",
    "    # TODO: using the hmm tagger evaluate on the test data\n",
    "    eres = tagger.evaluate(test_data)\n",
    "\n",
    "    return tagger, hmm_tagged_sentence, eres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentenced tagged with nltk.HiddenMarkovModelTagger:\n",
      "[('The', 'AT'), (\"mayor's\", 'QL'), ('present', 'JJ'), ('term', 'NN'), ('of', 'IN'), ('office', 'NN'), ('expires', '.'), ('Jan.', '.'), ('1', '.'), ('.', '.')]\n",
      "Eval score:\n",
      "0.7147266313932981\n",
      "Sentenced tagged with nltk.HiddenMarkovModelTagger:\n",
      "[('The', 'AT'), (\"mayor's\", 'NN$'), ('present', 'JJ'), ('term', 'NN'), ('of', 'IN'), ('office', 'NN'), ('expires', 'IN'), ('Jan.', 'NP'), ('1', 'CD'), ('.', '.')]\n",
      "Eval score:\n",
      "0.8686067019400353\n"
     ]
    }
   ],
   "source": [
    "# Test your code for exercise 2\n",
    "def test_ex2():\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "    words = [tp[0] for tp in tagged_sentences[42]]\n",
    "    tagger, hmm_tagged_sentence, eres = ex2(words, 500)\n",
    "    print('Sentenced tagged with nltk.HiddenMarkovModelTagger:')\n",
    "    print(hmm_tagged_sentence)\n",
    "    print('Eval score:')\n",
    "    print(eres)\n",
    "\n",
    "    tagger, hmm_tagged_sentence, eres = ex2(words, 3000)\n",
    "    print('Sentenced tagged with nltk.HiddenMarkovModelTagger:')\n",
    "    print(hmm_tagged_sentence)\n",
    "    print('Eval score:')\n",
    "    print(eres)\n",
    "\n",
    "#Look at the tagged sentence and the accuracy of the tagger. How does the size of the training set affect the accuracy?\n",
    "test_ex2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Transition and Emission Probabilities\n",
    "\n",
    "In the previous exercise we learned how to train and evaluate an HMM tagger.\n",
    "We have used the HMM tagger as a black box and have seen how the training\n",
    "data affects the accuracy of the tagger. In order to get a better understanding\n",
    "of the HMM we will look at the two components of this model:\n",
    "    \n",
    "* The transition model\n",
    "* The emission model\n",
    "\n",
    "The transition model estimates $P (tag_{i+1} |tag_i )$, the probability of a POS tag\n",
    "at position $i+1$ given the previous tag (at position $i$). The emission model\n",
    "estimates $P (word|tag)$, the probability of the observed word given a tag.\n",
    "\n",
    "Given the above definitions, we will need to learn a Conditional Probability\n",
    "Distribution for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ConditionalProbDist in module nltk.probability:\n",
      "\n",
      "class ConditionalProbDist(ConditionalProbDistI)\n",
      " |  ConditionalProbDist(cfdist, probdist_factory, *factory_args, **factory_kw_args)\n",
      " |  \n",
      " |  A conditional probability distribution modeling the experiments\n",
      " |  that were used to generate a conditional frequency distribution.\n",
      " |  A ConditionalProbDist is constructed from a\n",
      " |  ``ConditionalFreqDist`` and a ``ProbDist`` factory:\n",
      " |  \n",
      " |  - The ``ConditionalFreqDist`` specifies the frequency\n",
      " |    distribution for each condition.\n",
      " |  - The ``ProbDist`` factory is a function that takes a\n",
      " |    condition's frequency distribution, and returns its\n",
      " |    probability distribution.  A ``ProbDist`` class's name (such as\n",
      " |    ``MLEProbDist`` or ``HeldoutProbDist``) can be used to specify\n",
      " |    that class's constructor.\n",
      " |  \n",
      " |  The first argument to the ``ProbDist`` factory is the frequency\n",
      " |  distribution that it should model; and the remaining arguments are\n",
      " |  specified by the ``factory_args`` parameter to the\n",
      " |  ``ConditionalProbDist`` constructor.  For example, the following\n",
      " |  code constructs a ``ConditionalProbDist``, where the probability\n",
      " |  distribution for each condition is an ``ELEProbDist`` with 10 bins:\n",
      " |  \n",
      " |      >>> from nltk.corpus import brown\n",
      " |      >>> from nltk.probability import ConditionalFreqDist\n",
      " |      >>> from nltk.probability import ConditionalProbDist, ELEProbDist\n",
      " |      >>> cfdist = ConditionalFreqDist(brown.tagged_words()[:5000])\n",
      " |      >>> cpdist = ConditionalProbDist(cfdist, ELEProbDist, 10)\n",
      " |      >>> cpdist['passed'].max()\n",
      " |      'VBD'\n",
      " |      >>> cpdist['passed'].prob('VBD')\n",
      " |      0.423...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConditionalProbDist\n",
      " |      ConditionalProbDistI\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, cfdist, probdist_factory, *factory_args, **factory_kw_args)\n",
      " |      Construct a new conditional probability distribution, based on\n",
      " |      the given conditional frequency distribution and ``ProbDist``\n",
      " |      factory.\n",
      " |      \n",
      " |      :type cfdist: ConditionalFreqDist\n",
      " |      :param cfdist: The ``ConditionalFreqDist`` specifying the\n",
      " |          frequency distribution for each condition.\n",
      " |      :type probdist_factory: class or function\n",
      " |      :param probdist_factory: The function or class that maps\n",
      " |          a condition's frequency distribution to its probability\n",
      " |          distribution.  The function is called with the frequency\n",
      " |          distribution as its first argument,\n",
      " |          ``factory_args`` as its remaining arguments, and\n",
      " |          ``factory_kw_args`` as keyword arguments.\n",
      " |      :type factory_args: (any)\n",
      " |      :param factory_args: Extra arguments for ``probdist_factory``.\n",
      " |          These arguments are usually used to specify extra\n",
      " |          properties for the probability distributions of individual\n",
      " |          conditions, such as the number of bins they contain.\n",
      " |      :type factory_kw_args: (any)\n",
      " |      :param factory_kw_args: Extra keyword arguments for ``probdist_factory``.\n",
      " |  \n",
      " |  __missing__(self, key)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ConditionalProbDistI:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this ``ConditionalProbDist``.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  conditions(self)\n",
      " |      Return a list of the conditions that are represented by\n",
      " |      this ``ConditionalProbDist``.  Use the indexing operator to\n",
      " |      access the probability distribution for a given condition.\n",
      " |      \n",
      " |      :rtype: list\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |      Return a string representation of this ``ConditionalProbDist``.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from ConditionalProbDistI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.dict:\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from abc.ABCMeta\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.probability.ConditionalProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "In this exercise we will estimate the emission model. In order to compute the\n",
    "Conditional Probability Distribution of $P (word|tag)$ we first have to compute\n",
    "the Conditional Frequency Distribution of a word given a tag.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ConditionalFreqDist in module nltk.probability:\n",
      "\n",
      "class ConditionalFreqDist(collections.defaultdict)\n",
      " |  ConditionalFreqDist(cond_samples=None)\n",
      " |  \n",
      " |  A collection of frequency distributions for a single experiment\n",
      " |  run under different conditions.  Conditional frequency\n",
      " |  distributions are used to record the number of times each sample\n",
      " |  occurred, given the condition under which the experiment was run.\n",
      " |  For example, a conditional frequency distribution could be used to\n",
      " |  record the frequency of each word (type) in a document, given its\n",
      " |  length.  Formally, a conditional frequency distribution can be\n",
      " |  defined as a function that maps from each condition to the\n",
      " |  FreqDist for the experiment under that condition.\n",
      " |  \n",
      " |  Conditional frequency distributions are typically constructed by\n",
      " |  repeatedly running an experiment under a variety of conditions,\n",
      " |  and incrementing the sample outcome counts for the appropriate\n",
      " |  conditions.  For example, the following code will produce a\n",
      " |  conditional frequency distribution that encodes how often each\n",
      " |  word type occurs, given the length of that word type:\n",
      " |  \n",
      " |      >>> from nltk.probability import ConditionalFreqDist\n",
      " |      >>> from nltk.tokenize import word_tokenize\n",
      " |      >>> sent = \"the the the dog dog some other words that we do not care about\"\n",
      " |      >>> cfdist = ConditionalFreqDist()\n",
      " |      >>> for word in word_tokenize(sent):\n",
      " |      ...     condition = len(word)\n",
      " |      ...     cfdist[condition][word] += 1\n",
      " |  \n",
      " |  An equivalent way to do this is with the initializer:\n",
      " |  \n",
      " |      >>> cfdist = ConditionalFreqDist((len(word), word) for word in word_tokenize(sent))\n",
      " |  \n",
      " |  The frequency distribution for each condition is accessed using\n",
      " |  the indexing operator:\n",
      " |  \n",
      " |      >>> cfdist[3]\n",
      " |      FreqDist({'the': 3, 'dog': 2, 'not': 1})\n",
      " |      >>> cfdist[3].freq('the')\n",
      " |      0.5\n",
      " |      >>> cfdist[3]['dog']\n",
      " |      2\n",
      " |  \n",
      " |  When the indexing operator is used to access the frequency\n",
      " |  distribution for a condition that has not been accessed before,\n",
      " |  ``ConditionalFreqDist`` creates a new empty FreqDist for that\n",
      " |  condition.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConditionalFreqDist\n",
      " |      collections.defaultdict\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  N(self)\n",
      " |      Return the total number of sample outcomes that have been\n",
      " |      recorded by this ``ConditionalFreqDist``.\n",
      " |      \n",
      " |      :rtype: int\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Add counts from two ConditionalFreqDists.\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |      Intersection is the minimum of corresponding counts.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __init__(self, cond_samples=None)\n",
      " |      Construct a new empty conditional frequency distribution.  In\n",
      " |      particular, the count for every sample, under every condition,\n",
      " |      is zero.\n",
      " |      \n",
      " |      :param cond_samples: The samples to initialize the conditional\n",
      " |          frequency distribution with\n",
      " |      :type cond_samples: Sequence of (condition, sample) tuples\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Union is the maximum of value in either of the input counters.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Return state information for pickling.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this ``ConditionalFreqDist``.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |      Subtract count, but keep only results with positive counts.\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  conditions(self)\n",
      " |      Return a list of the conditions that have been accessed for\n",
      " |      this ``ConditionalFreqDist``.  Use the indexing operator to\n",
      " |      access the frequency distribution for a given condition.\n",
      " |      Note that the frequency distributions for some conditions\n",
      " |      may contain zero sample outcomes.\n",
      " |      \n",
      " |      :rtype: list\n",
      " |  \n",
      " |  plot(self, *args, **kwargs)\n",
      " |      Plot the given samples from the conditional frequency distribution.\n",
      " |      For a cumulative plot, specify cumulative=True.\n",
      " |      (Requires Matplotlib to be installed.)\n",
      " |      \n",
      " |      :param samples: The samples to plot\n",
      " |      :type samples: list\n",
      " |      :param title: The title for the graph\n",
      " |      :type title: str\n",
      " |      :param conditions: The conditions to plot (default is all)\n",
      " |      :type conditions: list\n",
      " |  \n",
      " |  tabulate(self, *args, **kwargs)\n",
      " |      Tabulate the given samples from the conditional frequency distribution.\n",
      " |      \n",
      " |      :param samples: The samples to plot\n",
      " |      :type samples: list\n",
      " |      :param conditions: The conditions to plot (default is all)\n",
      " |      :type conditions: list\n",
      " |      :param cumulative: A flag to specify whether the freqs are cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.defaultdict:\n",
      " |  \n",
      " |  __copy__(...)\n",
      " |      D.copy() -> a shallow copy of D.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __missing__(...)\n",
      " |      __missing__(key) # Called by __getitem__ for missing key; pseudo-code:\n",
      " |      if self.default_factory is None: raise KeyError((key,))\n",
      " |      self[key] = value = self.default_factory()\n",
      " |      return value\n",
      " |  \n",
      " |  copy(...)\n",
      " |      D.copy() -> a shallow copy of D.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.defaultdict:\n",
      " |  \n",
      " |  default_factory\n",
      " |      Factory for default value called by __missing__().\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      x.__getitem__(y) <==> x[y]\n",
      " |  \n",
      " |  __iter__(self, /)\n",
      " |      Implement iter(self).\n",
      " |  \n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |  \n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |  \n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      " |  \n",
      " |  popitem(...)\n",
      " |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      " |      2-tuple; but raise KeyError if D is empty.\n",
      " |  \n",
      " |  setdefault(self, key, default=None, /)\n",
      " |      Insert key with a value of default if key is not in the dictionary.\n",
      " |      \n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |  \n",
      " |  update(...)\n",
      " |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      " |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      " |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      " |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      " |  \n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.dict:\n",
      " |  \n",
      " |  fromkeys(iterable, value=None, /) from builtins.type\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from builtins.dict:\n",
      " |  \n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.probability.ConditionalFreqDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor of the ConditionalFreqDist class takes as input a list of tuples,\n",
    "each tuple consisting of a condition and an observation. For the emission model,\n",
    "the conditions are tags and the observations are the words. The template of the\n",
    "function that you have to implement takes as argument the list of tagged words\n",
    "from the Brown corpus.\n",
    "\n",
    "1. Build the dataset to be passed to the `ConditionalFreqDist()` constructor. Words should be lowercased. Each item of data should be a tuple of tag (a condition) and word (an observation).\n",
    "2. Compute the Conditional Frequency Distribution of words given tags.\n",
    "3. Return the top 10 most frequent words given the tag NN.\n",
    "4. Compute the Conditional Probability Distribution for the above Conditional Frequency Distribution. Use the `MLEProbDist` estimator when calling the ConditionalProbDist constructor.\n",
    "5. Compute the probabilities:\n",
    "\n",
    " $P(\\text{year}|\\text{NN})$ \n",
    " \n",
    " $P(\\text{year}|\\text{DT})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############# EXERCISE 3 #################\n",
    "\n",
    "# Solution for exercise 3\n",
    "\n",
    "def ex3(tagged_words):\n",
    "    \"\"\"Build and sample Conditional{Freq->Prob}Dist for word given tag from a list of tagged words using MLE\n",
    "    \n",
    "    :param tagged_words: tagged words (word,tag)\n",
    "    :type tagged_words: list(tuple(str,str))\n",
    "    :return: Conditional Freq dist of word given tag, top 10 words with tag NN,\n",
    "             Conditional Prob dist of word given tag, P('year'|'NN'), P('year'|'DT')\n",
    "    :rtype: tuple(nltk.probability.ConditionalFreqDist,list(tuple(str,int)),nltk.probability.ConditionalProbDist,float,float)\"\"\"\n",
    "    \n",
    "    # in the previous labs we've seen how to build a freq dist\n",
    "    # we need conditional distributions to estimate the transition and emission models\n",
    "    # in this exercise we estimate the emission model\n",
    "    \n",
    "    # TODO: prepare the data\n",
    "    # the data object should be a list of tuples of conditions and observations\n",
    "    # in our case the tuples should be of the form (tag,word) where words are lowercased\n",
    "    data = [ (tag, word.lower()) for (word, tag) in tagged_words]\n",
    "\n",
    "    # TODO: compute a Conditional Frequency Distribution for words given their tags using our data\n",
    "    emission_FD =ConditionalFreqDist(data)\n",
    "    \n",
    "    # TODO: find the top 10 most frequent words given the tag NN\n",
    "    top_NN = emission_FD['NN'].most_common(10)\n",
    "    \n",
    "    # TODO: Compute the Conditional Probability Distribution using the above Conditional Frequency Distribution.\n",
    "    #  Use MLE Prob Distro estimator.\n",
    "    emission_PD =ConditionalProbDist(emission_FD,MLEProbDist)\n",
    "    \n",
    "    # TODO: compute the probabilities of P(year|NN) and P(year|DT)\n",
    "    p_NN = emission_PD['NN'].prob('year')\n",
    "    p_DT = emission_PD['DT'].prob('year')\n",
    "    \n",
    "    return emission_FD, top_NN, emission_PD, p_NN, p_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of words given the tag *NN*:  [('year', 137), ('time', 98), ('state', 92), ('week', 86), ('man', 72), ('home', 72), ('program', 65), ('school', 65), ('night', 64), ('day', 62)]\n",
      "P(year|NN) =  0.0104087524692296\n",
      "P(year|DT) =  0.0\n"
     ]
    }
   ],
   "source": [
    "### Test your solution for exercise 3\n",
    "\n",
    "def test_ex3():\n",
    "    tagged_words = brown.tagged_words(categories='news')\n",
    "    (emission_FD, top_NN, emission_PD, p_NN, p_DT) = ex3(tagged_words)\n",
    "    print('Frequency of words given the tag *NN*: ', top_NN)\n",
    "    print('P(year|NN) = ', p_NN)\n",
    "    print('P(year|DT) = ', p_DT)\n",
    "\n",
    "# Look at the estimated probabilities. Why is P(year|DT) = 0 ? \n",
    "# What are the problems with having 0 probabilities and what can be done to avoid this?\n",
    "test_ex3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the problems with having zero probabilities and what can be done to\n",
    "avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "In this exercise we will estimate the transition model. In order to compute the\n",
    "Conditional Probability Distribution of $P (tag_{i+1} |tag_i )$ we first have to compute\n",
    "the Conditional Frequency Distribution of a tag at position $i + 1$ given the previous tag.\n",
    "\n",
    "The constructor of the `ConditionalFreqDist` class takes as input a list of tuples, each tuple consisting of a condition and an observation. For the transition\n",
    "model, the conditions are tags at position i and the observations are tags at\n",
    "position $i + 1$. The template of the function that you have to implement takes\n",
    "as argument the list of tagged sentences from the Brown corpus.\n",
    "\n",
    "1. Build the dataset to be passed to the `ConditionalFreqDist()` constructor. Each item in your data should be a pair of condition and observation: $(tag_i,tag_{i+1})$\n",
    "2. Compute the Conditional Frequency Distribution of a tag at position $i + 1$ given the previous tag.\n",
    "3. Compute the Conditional Probability Distribution for the above Conditional Frequency Distribution. Use the `MLEProbDist` estimator when calling the `ConditionalProbDist` constructor.\n",
    "4. Compute the probabilities \n",
    "   \n",
    "   $P(\\text{NN}|\\text{VBD})$ \n",
    "   \n",
    "   $P(\\text{NN}|\\text{DT})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# EXERCISE 4 #################\n",
    "\n",
    "# Solution for exercise 4\n",
    "\n",
    "def ex4(tagged_sentences):\n",
    "    \"\"\"Build and sample Conditional{Freq->Prob}Dist for tag given preceding tag from a list of tagged words using MLE\n",
    "    \n",
    "    :param tagged_sentences: Tagged sentences for training and testing\n",
    "    :type tagged_sentencts: list(list(str))\n",
    "    :return: Conditional Freq dist of tag given preceding tag,\n",
    "             Conditional Prob dist of tag given preceding tag, P('NN'|'VBD') and P('NN'|'DT')\n",
    "    :rtype: tuple(nltk.probability.ConditionalFreqDist,nltk.probability.ConditionalProbDist,float,float)\"\"\"\n",
    "\n",
    "    # TODO: prepare the data\n",
    "    # the data object should be an array of tuples of conditions and observations\n",
    "    # in our case the tuples will be of the form (tag_(i),tag_(i+1))\n",
    "    tagGenerators=(((s[i][1],s[i+1][1]) for i in range(len(s)-1)) for s in tagged_sentences)\n",
    "    # tagGenerators is an iterator of iterators of pairs of tags\n",
    "    # The following chains them all together to produce an iterator of pairs of tags\n",
    "    data = itertools.chain.from_iterable(tagGenerators)\n",
    "\n",
    "    # TODO: compute a Conditional Frequency Distribution for a tag given the previous tag\n",
    "    transition_FD =ConditionalFreqDist(data)\n",
    "    \n",
    "    # TODO: compute a Conditional Probability Distribution for the\n",
    "    # transition probability P(tag_(i+1)|tag_(i)) using an MLEProbDist\n",
    "    # to estimate the probabilities\n",
    "    transition_PD =ConditionalProbDist(transition_FD, MLEProbDist)\n",
    "\n",
    "    # TODO: compute the probabilities of P('NN'|'VBD') and P('NN'|'DT')\n",
    "    p_VBD_NN = transition_PD['VBD'].prob('NN')\n",
    "    p_DT_NN = transition_PD['DT'].prob('NN')\n",
    "\n",
    "    return transition_FD, transition_PD, p_VBD_NN, p_DT_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(NN|VBD) =  0.028526148969889066\n",
      "P(NN|DT) =  0.5289115646258503\n"
     ]
    }
   ],
   "source": [
    "### Test your solution for exercise 4\n",
    "\n",
    "def test_ex4():\n",
    "    tagged_sentences = brown.tagged_sents(categories='news')\n",
    "    (transition_FD, transition_PD, p_VBD_NN, p_DT_NN) = ex4(tagged_sentences)\n",
    "    print('P(NN|VBD) = ', p_VBD_NN)\n",
    "    print('P(NN|DT) = ', p_DT_NN)\n",
    "    \n",
    "# Are the results what you would expect? The sequence NN DT seems very probable. \n",
    "# How will this affect the sequence tagging?\n",
    "test_ex4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "Modify your code for exercise 3 to use a different estimator, to introduce some\n",
    "smoothing, and compare the results with the original.\n",
    "In exercise 4 we didn’t do anything about the boundaries. Modify your code for\n",
    "exercise 4 to use `<s>` at the beginning of every sentence and `</s>` at the end.\n",
    "\n",
    "Explore the resulting conditional probabilities. What is the most likely tag at\n",
    "the beginning of a sentence? At the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
