{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNLP 2019: Lab Session 5: Word Sense Disambiguation\n",
    "\n",
    "##  Word Sense Disambiguation: Recap\n",
    "\n",
    "In this tutorial we will be exploring the lexical sample task. This is a task where you use a corpus to learn how to disambiguate a small set of target words using supervised learning. The aim is to build a classifier that maps each occurrence of a target word in a corpus to its sense.\n",
    "\n",
    "We will use a Naive Bayes classifier. In other words, where the context of an occurrence of a target word in the corpus is represented as a feature vector, the classifier estimates the word sense s on the basis of its context as shown below. \n",
    "\n",
    "\n",
    "![Slide from lecture 14](nb_maths.jpg)\n",
    "\n",
    "## The corpus\n",
    "\n",
    "We will use the [senseval-2](http://www.hipposmond.com/senseval2) corpus for our training and test data. This corpus consists of text from a mixture of places, including the British National Corpus and the Penn Treebank portion of the Wall Street Journal. Each word in the corpus is tagged with its part of speech, and the senses of the following target words are also manually annotated: the nouns *interest*, *line*; the verb *serve* and the adjective *hard*. You can find out more about the task from [here](http://www.hipposmond.com/senseval2/descriptions/english-lexsample.htm).\n",
    "\n",
    "The sets of senses that are used to annotate each target word come from WordNet (more on that later).\n",
    "\n",
    "## Getting started: Run the code\n",
    "\n",
    "Look at the code below, and try to understand how it works (don't worry if you don't understand some of it, it's not necessary for doing this task).\n",
    "    Remember, `help(...)` is your friend:\n",
    "  * `help([class name])` for classes and all their methods and instance variables\n",
    "  * `help([any object])` likewise\n",
    "  * `help([function])` or `help([class].[method])` for functions / methods\n",
    "\n",
    "This code allows you to do several things. You can now run, train and evaluate a range of Naive Bayes classifiers over the corpus to acquire a model of WSD for a given target word: the adjective *hard*, the nouns *interest* or *line*, and the verb *serve*. We'll learn later how you do this. First, we're going to explore the nature of the corpus itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import senseval\n",
    "from nltk.classify import accuracy, NaiveBayesClassifier, MaxentClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "# The following shows how the senseval corpus consists of instances, where each instance\n",
    "# consists of a target word (and its tag), it position in the sentence it appeared in\n",
    "# within the corpus (that position being word position, minus punctuation), and the context,\n",
    "# which is the words in the sentence plus their tags.\n",
    "#\n",
    "# senseval.instances()[:1]\n",
    "# [SensevalInstance(word='hard-a', position=20, context=[('``', '``'), ('he', 'PRP'),\n",
    "# ('may', 'MD'), ('lose', 'VB'), ('all', 'DT'), ('popular', 'JJ'), ('support', 'NN'),\n",
    "#Â (',', ','), ('but', 'CC'), ('someone', 'NN'), ('has', 'VBZ'), ('to', 'TO'),\n",
    "# ('kill', 'VB'), ('him', 'PRP'), ('to', 'TO'), ('defeat', 'VB'), ('him', 'PRP'),\n",
    "# ('and', 'CC'), ('that', 'DT'), (\"'s\", 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('do', 'VB'),\n",
    "# ('.', '.'), (\"''\", \"''\")], senses=('HARD1',))]\n",
    "\n",
    "def senses(word):\n",
    "    \"\"\"Return the list of possible senses for a word per senseval-2\n",
    "    \n",
    "    :param word: The word to look up\n",
    "    :type word: str\n",
    "    :return: list of senses\n",
    "    :rtype: list(str)\n",
    "    \"\"\"\n",
    "    return list(set(i.senses[0] for i in senseval.instances(word)))\n",
    "\n",
    "# Both above and below, we depend on the (non-obvious?) fact that although the field is\n",
    "#  called 'senses', there is always only 1, i.e. there is no residual ambiguity in the\n",
    "#  data as we have it, because this is the gold standard and disambiguation per\n",
    "#  the context has already been done\n",
    "\n",
    "def sense_instances(instances, sense):\n",
    "    \"\"\"Return a list of instances that have the given sense\n",
    "    \n",
    "    :param instances: corpus of sense-labelled instances\n",
    "    :type instances: list(senseval.SensevalInstance)\n",
    "    :param sense: The target sense\n",
    "    :type sense: str\n",
    "    :return: matching instances\n",
    "    :rtype: list(senseval.SensevalInstance)\n",
    "    \"\"\"\n",
    "    return [instance for instance in instances if instance.senses[0]==sense]\n",
    "\n",
    "# >>> sense3 = sense_instances(senseval.instances('hard.pos'), 'HARD3')\n",
    "# >>> sense3[:2]\n",
    "# [SensevalInstance(word='hard-a', position=15,\n",
    "#  context=[('my', 'PRP$'), ('companion', 'NN'), ('enjoyed', 'VBD'), ('a', 'DT'), ('healthy', 'JJ'), ('slice', 'NN'), ('of', 'IN'), ('the', 'DT'), ('chocolate', 'NN'), ('mousse', 'NN'), ('cake', 'NN'), (',', ','), ('made', 'VBN'), ('with', 'IN'), ('a', 'DT'), ('hard', 'JJ'), ('chocolate', 'NN'), ('crust', 'NN'), (',', ','), ('topping', 'VBG'), ('a', 'DT'), ('sponge', 'NN'), ('cake', 'NN'), ('with', 'IN'), ('either', 'DT'), ('strawberry', 'NN'), ('or', 'CC'), ('raspberry', 'JJ'), ('on', 'IN'), ('the', 'DT'), ('bottom', 'NN'), ('.', '.')],\n",
    "#  senses=('HARD3',)),\n",
    "#  SensevalInstance(word='hard-a', position=5,\n",
    "#  context=[('``', '``'), ('i', 'PRP'), ('feel', 'VBP'), ('that', 'IN'), ('the', 'DT'), ('hard', 'JJ'), ('court', 'NN'), ('is', 'VBZ'), ('my', 'PRP$'), ('best', 'JJS'), ('surface', 'NN'), ('overall', 'JJ'), (',', ','), ('\"', '\"'), ('courier', 'NNP'), ('said', 'VBD'), ('.', '.')],\n",
    "# senses=('HARD3',))]\n",
    "\n",
    "_inst_cache = {}\n",
    "\n",
    "STOPWORDS = ['.', ',', '?', '\"', '``', \"''\", \"'\", '--', '-', ':', ';', '(',\n",
    "             ')', '$', '000', '1', '2', '10,' 'I', 'i', 'a', 'about', 'after', 'all', 'also', 'an', 'any',\n",
    "             'are', 'as', 'at', 'and', 'be', 'being', 'because', 'been', 'but', 'by',\n",
    "             'can', \"'d\", 'did', 'do', \"don'\", 'don', 'for', 'from', 'had','has', 'have', 'he',\n",
    "             'her','him', 'his', 'how', 'if', 'is', 'in', 'it', 'its', \"'ll\", \"'m\", 'me',\n",
    "             'more', 'my', 'n', 'no', 'not', 'of', 'on', 'one', 'or', \"'re\", \"'s\", \"s\",\n",
    "             'said', 'say', 'says', 'she', 'so', 'some', 'such', \"'t\", 'than', 'that', 'the',\n",
    "             'them', 'they', 'their', 'there', 'this', 'to', 'up', 'us', \"'ve\", 'was', 'we', 'were',\n",
    "             'what', 'when', 'where', 'which', 'who', 'will', 'with', 'years', 'you',\n",
    "             'your']\n",
    "\n",
    "STOPWORDS_SET=set(STOPWORDS)\n",
    "\n",
    "NO_STOPWORDS = []\n",
    "\n",
    "def wsd_context_features(instance, vocab, dist=3):\n",
    "    \"\"\"Return a featureset dictionary of left/right context word features within a distance window\n",
    "    of the sense-classified word of a senseval-2 instance, also a feature for the word and for\n",
    "    its part of speech,\n",
    "    for use by an NLTK classifier such as NaiveBayesClassifier or MaxentClassifier\n",
    "    \n",
    "    :param instance: sense-labelled instance to extract features from\n",
    "    :type instance: senseval.SensevalInstance\n",
    "    :param vocab: ignored in this case\n",
    "    :type vocab: str\n",
    "    :param dist: window size\n",
    "    :type dist: int\n",
    "    :return: feature dictionary\n",
    "    :rtype: dict\"\"\"\n",
    "    features = {}\n",
    "    ind = instance.position\n",
    "    con = instance.context\n",
    "    for i in range(max(0, ind-dist), ind):\n",
    "        j = ind-i\n",
    "        features['left-context-word-%s(%s)' % (j, con[i][0])] = True\n",
    "\n",
    "    for i in range(ind+1, min(ind+dist+1, len(con))):\n",
    "        j = i-ind\n",
    "        features['right-context-word-%s(%s)' % (j, con[i][0])] = True\n",
    "\n",
    "    features['word'] = instance.word\n",
    "    features['pos'] = con[1][1]\n",
    "    return features\n",
    "\n",
    "def wsd_word_features(instance, vocab, dist=3):\n",
    "    \"\"\"Return a featureset for an NLTK classifier such as NaiveBayesClassifier or MaxentClassifier\n",
    "    where every key returns False unless it occurs in the instance's context\n",
    "    and in a specified vocabulary\n",
    "    \n",
    "    :param instance: sense-labelled instance to extract features from\n",
    "    :type instance: senseval.SensevalInstance\n",
    "    :param vocab: filter for context words that yield features\n",
    "    :type vocab: list(str)\n",
    "    :param dist: ignored in this case\n",
    "    :type dist: int\n",
    "    :return: feature dictionary\n",
    "    :rtype: dict\"\"\"\n",
    "    features = defaultdict(lambda:False)\n",
    "    features['alwayson'] = True\n",
    "    # Not all context items are (word,pos) pairs, for some reason some are just strings...\n",
    "    for w in (e[0] for e in instance.context if isinstance(e,tuple)):\n",
    "            if w in vocab:\n",
    "                features[w] = True\n",
    "    return features\n",
    "\n",
    "def extract_vocab_frequency(instances, stopwords=STOPWORDS_SET, n=300):\n",
    "    \"\"\"Construct a frequency distribution of the non-stopword context words\n",
    "    in a collection of senseval-2 instances and return the top n entries, sorted\n",
    "    \n",
    "    :param instances: sense-labelled instances to extract from\n",
    "    :type instance: list(senseval.SensevalInstance)\n",
    "    :param stopwords: words to exclude from the result\n",
    "    :type stopwords: iterable(string)\n",
    "    :param n: number of items to return\n",
    "    :type n: int\n",
    "    :return: sorted list of at most n items from the frequency distribution\n",
    "    :rtype: list(tuple(str,int))\n",
    "    \"\"\"\n",
    "    fd = nltk.FreqDist()\n",
    "    for i in instances:\n",
    "        (target, suffix) = i.word.split('-')\n",
    "        words = (c[0] for c in i.context if not c[0] == target)\n",
    "        for word in set(words) - set(stopwords):\n",
    "            fd[word] += 1\n",
    "    return fd.most_common()[:n+1]\n",
    "        \n",
    "def extract_vocab(instances, stopwords=STOPWORDS_SET, n=300):\n",
    "    \"\"\"Return the n most common non-stopword words appearing as context\n",
    "    in a collection of semeval-2 instances\n",
    "    \n",
    "    A wrapper for extract_vocab_frequency, q.v.\n",
    "    \n",
    "    :param instances: sense-labelled instances to extract from\n",
    "    :type instance: list(senseval.SensevalInstance)\n",
    "    :param stopwords: words to exclude from the result\n",
    "    :type stopwords: iterable(string)\n",
    "    :param n: number of words to return\n",
    "    :type n: int\n",
    "    :return: sorted list of at most n words\n",
    "    :rtype: list(str)\"\"\"\n",
    "\n",
    "    return [w for w,f in extract_vocab_frequency(instances,stopwords,n)]\n",
    "    \n",
    "def wst_classifier(trainer, word, features, stopwords_list = STOPWORDS_SET, number=300, log=False, distance=3, confusion_matrix=False):\n",
    "    \"\"\"Build a classifier instance for the senseval2 senses of a word and applies it\n",
    "    \n",
    "    :param trainer: the trainer class method for an NLTK classifier such as NaiveBayesClassifier or MaxentClassifier\n",
    "    :type trainer: method(list(tuple(featureset,label)))\n",
    "    :param word: from senseval2 (we have 'hard.pos', 'interest.pos', 'line.pos' and 'serve.pos')\n",
    "    :type string:\n",
    "    :param features: a feature set constructor (we have wsd_context_features or wsd_word_features)\n",
    "    :type features: function(senseval.SensevalInstance,list(str),int)\n",
    "    :param number: passed to extract_vocab when constructing the second argument to the feature set constructor\n",
    "    :type int:\n",
    "    :param log: if set to True outputs any errors into a file errors.txt\n",
    "    :type bool:\n",
    "    :param distance: passed to the feature set constructor as 3rd argument\n",
    "    :type int:\n",
    "    :param confusion_matrix: if set to True prints a confusion matrix\n",
    "    :type bool:\n",
    "\n",
    "    Calling this function splits the senseval data for the word into a training set and a test set (the way it does\n",
    "    this is the same for each call of this function, because the argument to random.seed is specified,\n",
    "    but removing this argument would make the training and testing sets different each time you build a classifier).\n",
    "\n",
    "    It then trains the trainer on the training set to create a classifier that performs WSD on the word,\n",
    "    using features (with number or distance where relevant).\n",
    "\n",
    "    It then tests the classifier on the test set, and prints its accuracy on that set.\n",
    "\n",
    "    If log==True, then the errors of the classifier over the test set are written to errors.txt.\n",
    "    For each error four things are recorded: (i) the example number within the test data (this is simply the index of the\n",
    "    example within the list test_data); (ii) the sentence that the target word appeared in, (iii) the\n",
    "    (incorrect) derived label, and (iv) the gold label.\n",
    "\n",
    "    If confusion_matrix==True, then calling this function prints out a confusion matrix, where each cell [i,j]\n",
    "    indicates how often label j was predicted when the correct label was i (so the diagonal entries indicate labels\n",
    "    that were correctly predicted).\n",
    "    \"\"\"\n",
    "    print(\"Reading data...\")\n",
    "    global _inst_cache\n",
    "    if word not in _inst_cache:\n",
    "        _inst_cache[word] = [(i, i.senses[0]) for i in senseval.instances(word)]\n",
    "    events = _inst_cache[word][:]\n",
    "    senses = list(set(l for (i, l) in events))\n",
    "    instances = [i for (i, l) in events]\n",
    "    vocab = extract_vocab(instances, stopwords=stopwords_list, n=number)\n",
    "    print(' Senses: ' + ' '.join(senses))\n",
    "\n",
    "    # Split the instances into a training and test set,\n",
    "    #if n > len(events): n = len(events)\n",
    "    n = len(events)\n",
    "    random.seed(5043562) # 5444522\n",
    "    random.shuffle(events)\n",
    "    training_data = events[:int(0.8 * n)]\n",
    "    test_data = events[int(0.8 * n):n]\n",
    "    # Train classifier\n",
    "    print('Training classifier...')\n",
    "    classifier = trainer([(features(i, vocab, distance), label) for (i, label) in training_data])\n",
    "    # Test classifier\n",
    "    print('Testing classifier...')\n",
    "    acc = accuracy(classifier, [(features(i, vocab, distance), label) for (i, label) in test_data] )\n",
    "    print('Accuracy: %6.4f' % acc)\n",
    "    if log:\n",
    "        #write error file\n",
    "        print('Writing errors to errors.txt')\n",
    "        output_error_file = open('errors.txt', 'w')\n",
    "        errors = []\n",
    "        for (i, label) in test_data:\n",
    "            guess = classifier.classify(features(i, vocab, distance))\n",
    "            if guess != label:\n",
    "                con =  i.context\n",
    "                position = i.position\n",
    "                item_number = str(test_data.index((i, label)))\n",
    "                word_list=[cv[0] if isinstance(cv,tuple) else cv for cv in con]\n",
    "                hard_highlighted = word_list[position].upper()\n",
    "                word_list_highlighted = word_list[0:position] + [hard_highlighted] + word_list[position+1:]\n",
    "                sentence = ' '.join(word_list_highlighted)\n",
    "                errors.append([item_number, sentence, guess,label])\n",
    "        error_number = len(errors)\n",
    "        output_error_file.write('There are ' + str(error_number) + ' errors!' + '\\n' + '----------------------------' +\n",
    "                                '\\n' + '\\n')\n",
    "        for error in errors:\n",
    "            output_error_file.write(str(errors.index(error)+1) +') ' + 'example number: ' + error[0] + '\\n' +\n",
    "                                    '    sentence: ' + error[1] + '\\n' +\n",
    "                                    '    guess: ' + error[2] + ';  label: ' + error[3] + '\\n' + '\\n')\n",
    "        output_error_file.close()\n",
    "    if confusion_matrix:\n",
    "        gold = [label for (i, label) in test_data]\n",
    "        derived = [classifier.classify(features(i,vocab)) for (i,label) in test_data]\n",
    "        cm = nltk.ConfusionMatrix(gold,derived)\n",
    "        print(cm)\n",
    "        return cm\n",
    "        \n",
    "def demo():\n",
    "    print(\"NB, with features based on 300 most frequent context words\")\n",
    "    wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features)\n",
    "    print(\"\\nNB, with features based word + pos in 6 word window\")\n",
    "    wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_context_features)\n",
    "##    print \"MaxEnt, with features based word + pos in 6 word window\"\n",
    "##    wst_classifier(MaxentClassifier.train, 'hard.pos', wsd_context_features)\n",
    "    \n",
    "#demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Senseval corpus\n",
    "## Target words\n",
    "\n",
    "You can find out the set of target words for the senseval-2 corpus by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senseval.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The result doesn't tell you the syntactic category of the words, but see the description of the corpus in Section 1 or Section 4.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word senses\n",
    "\n",
    "Let's now find out the set of word senses for each target word in senseval. There is a function in above that returns this information. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(senses('hard.pos'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this gives you `['HARD1', 'HARD2', 'HARD3']`\n",
    "\n",
    "So there are 3 senses for the adjective hard in the corpus. You'll shortly be looking at the data to guess what these 3 senses are.\n",
    "\n",
    "Now it's your turn:\n",
    "\n",
    "* What are the senses for the other target words? Find out by calling senses with appropriate arguments.\n",
    "* How many senses does each target have?\n",
    "* Let's now guess the sense definitions for HARD1, HARD2 and HARD3 by looking at the 100 most frequent open class words that occur in the context of each sense. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out what these 100 words for HARD1 by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "instances1 = sense_instances(senseval.instances('hard.pos'), 'HARD1')\n",
    "features1 = extract_vocab_frequency(instances1, n=100)\n",
    "\n",
    "# Now lets try printing features1:\n",
    "pprint(features1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn:\n",
    "\n",
    "* Call the above functions for HARD2 and HARD3.\n",
    "* Look at the resulting lists of 100 most frequent words for each sense, and try to define what HARD1, HARD2 and HARD3 mean.\n",
    "* These senses are actually the first three senses for the adjective _hard_ in [WordNet](http://wordnet.princeton.edu/). You can enter a word and get its list of WordNet senses from [here](http://wordnetweb.princeton.edu/perl/webwn). Do this for hard, and check whether your estimated definitions for the 3 word senses are correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances2 = sense_instances(senseval.instances('hard.pos'), 'HARD2')\n",
    "features2 = extract_vocab_frequency(instances2, n=20)\n",
    "\n",
    "instances3 = sense_instances(senseval.instances('hard.pos'), 'HARD3')\n",
    "features3 = extract_vocab_frequency(instances3, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data structures: Senseval instances\n",
    "Having extracted all instances of a given sense, you can look at what the data structures in the corpus look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For HARD2:\\nSample instance: %s\\nAll features:\"%instances2[0])\n",
    "pprint(features2)\n",
    "print(\"\\nFor HARD3:\\nSample instance: %s\\nAll features:\"%instances3[0])\n",
    "pprint(features3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So the senseval corpus is a collection of information about a set of tagged sentences, where each entry or instance consists of 4 attributes:\n",
    "\n",
    "* word specifies the target word together with its syntactic category (e.g., hard-a means that the word is hard and its category is 'adjective');\n",
    "* position gives its position within the sentence (ignoring punctuation);\n",
    "* context represents the sentence as a list of pairs, each pair being a word or punctuation mark and its tag; and finally\n",
    "* senses is a tuple, each item in the tuple being a sense for that target word. In the subset of the corpus we are working with, this tuple consists of only one argument. But there are a few examples elsewhere in the corpus where there is more than one, representing the fact that the annotator couldn't decide which sense to assign to the word. For simplicity, our classifiers are going to ignore any non-first arguments to the attribute senses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring different WSD classifiers\n",
    "You're now going to compare the performance of different classifiers that perform word sense disambiguation. You do this by calling the function `wst_classifer` This function must have at least the following arguments specified by you:\n",
    "\n",
    " 1. A trainer; e.g., `NaiveBayesClassifier.train` (if you want you could also try `MaxentClassifier.train`, but this takes longer to train).\n",
    " 2. The target word that the classifier is going to learn to disambiguate: i.e., 'hard.pos', 'line.pos', 'interest.pos' or 'serve.pos'.\n",
    " 3. A feature set. The code allows you to use two kinds of feature sets:\n",
    "#### wsd_word_features\n",
    "This feature set is based on the set **S&nbsp;** of the **n&nbsp;** most frequent words that occur in the same sentence as the target word **w&nbsp;** across the entire training corpus (as you'll see later, you can specify the value of **n&nbsp;**, but if you don't specify it then it defaults to 300). For each occurrence of **w,** `wsd_word_features` represents its context as the subset of those words from **S&nbsp;** that occur in the **w&nbsp;**'s sentence. By default, the closed-class words that are specified in `STOPWORDS` are excluded from the set **S&nbsp;** of most frequent words. But as we'll see later, you can also include closed-class words in **S&nbsp;**, or re-define closed-class words in any way you like! If you want to know what closed-class words are excluded by default, just look at the code above. \n",
    "#### wsd_context_features\n",
    "This feature set represents the context of a word **w&nbsp;** as the sequence of **m&nbsp;** pairs `(word,tag)` that occur before **w&nbsp;** and the sequence of **m&nbsp;** pairs `(word, tag)` that occur after **w&nbsp;**. As we'll see shortly, you can specify the value of **m&nbsp;** (e.g., `m=1` means the context consists of just the immediately prior and immediately subsequent word-tag pairs); otherwise, **m&nbsp;** defaults to 3. \n",
    "    \n",
    "    \n",
    "## Now let's train our first classifier\n",
    "Try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the adjective hard is tagged with 3 senses in the corpus (HARD1, HARD2 and HARD3), and the Naive Bayes Classifier using the feature set based on the 300 most frequent context words yields an accuracy of 0.8362. \n",
    "\n",
    "#### Now it's your turn:\n",
    "\n",
    "Use `wst_classifier` to train a classifier that disambiguates hard using `wsd_context_features`. Build classifiers for *line* and *serve* as well, using the word features and then the context features.\n",
    "\n",
    "* What's more accurate for disambiguating 'hard.pos', `wsd_context_features` or `wst_word_features`?\n",
    "* Does the same hold true for 'line.pos' and 'serve.pos'. Why do you think that might be?\n",
    "* Why is it not fair to compare the accuracy of the classifiers across different target words? \n",
    "\n",
    "    \n",
    "# Baseline models\n",
    "Just how good is the accuracy of these WSD classifiers? To find out, we need a baseline. There are two we consider here:\n",
    "\n",
    "1. A model which assigns a sense at random.\n",
    "2. A model which always assigns the most frequent sense. \n",
    "\n",
    "### Now it's your turn:\n",
    "\n",
    "* What is the accuracy of the random baseline model for 'hard.pos'?\n",
    "* To compute the accuracy of the frequency baseline model for 'hard.pos', we need to find out the Frequency Distribution of the three senses in the corpus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_sense_fd = nltk.FreqDist([i.senses[0] for i in senseval.instances('hard.pos')])\n",
    "print(hard_sense_fd.most_common())\n",
    "\n",
    "frequency_hard_sense_baseline = hard_sense_fd.freq('HARD1')\n",
    "frequency_hard_sense_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In other words, the frequency baseline has an accuracy of approx. 0.797. What is the most frequent sense for 'hard.pos'? And is the frequency baseline a better model than the random model?\n",
    "* Now compute the accuracy of the frequency baseline for other target words; e.g. 'line.pos'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_sense_fd = nltk.FreqDist([i.senses[0] for i in senseval.instances('line.pos')])\n",
    "print(line_sense_fd.most_common())\n",
    "\n",
    "frequency_line_sense_baseline = line_sense_fd.freq('product')\n",
    "frequency_line_sense_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rich features vs. sparse data\n",
    "In this part of the tutorial we are going to vary the feature sets and compare the results. As well as being able to choose between `wsd_context_features` vs. `wsd_word_features`, you can also vary the following:\n",
    "\n",
    "#### wsd_context_features\n",
    "\n",
    "You can vary the number of word-tag pairs before and after the target word that you include in the feature vector. You do this by specifying the argument `distance` to the function `wst_classifier`. For instance, the following creates a classifier that uses 2 words to the left and right of the target word: \n",
    "    \n",
    "    wst_classifier(NaiveBayesClassifier.train, 'hard.pos', \n",
    "\t\twsd_context_features, distance=2)\n",
    "\n",
    "What about distance 1?\n",
    "#### wsd_word_features\n",
    "You can vary the closed-class words that are excluded from the set of most frequent words, and you can vary the size of the set of most frequent words. For instance, the following results in a model which uses the 100 most frequent words including closed-class words:\n",
    "\n",
    "    wst_classifier(NaiveBayesClassifier.train, 'hard.pos', \n",
    "    \t\twsd_word_features, stopwords_list=[], number=100)\n",
    "           \n",
    "#### Now it's your turn:\n",
    "Build several WSD models for 'hard.pos', including at least the following: for the `wsd_word_features` version, vary `number` between 100, 200 and 300, and vary the `stopwords_list` between `[]` (i.e., the empty list) and `STOPWORDS`; for the `wsd_context_features` version, vary the `distance` between 1, 2 and 3, and vary the `stopwords_list` between `[]` and `STOPWORDS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [100, 200, 300, 400]:\n",
    "    for stopwords in [[], STOPWORDS]:\n",
    "        stop = 'stopwords' if stopwords else 'no stopwords'\n",
    "        print('Word features with number: %s and %s'%(n, stop))\n",
    "        wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features, number=n, stopwords_list=stopwords) \n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    for stopwords in [[], STOPWORDS]:\n",
    "        stop = 'stopwords' if stopwords else 'no stopwords'\n",
    "        print('Context features with distance: %s and %s'%(n, stop))\n",
    "        wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_context_features,stopwords_list=stopwords, distance=n) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does changing `number` have an inconsistent impact on the word model?\n",
    "  * This suggests that the data is too sparse for changes in vocabulary size to have a consistent impact.\n",
    "\n",
    "Why does making the context window before and after the target word to a number smaller than 3 improve the model?\n",
    "  * Sparse data, again\n",
    "\n",
    "Why does including closed-class words in word model improve overall performance?\n",
    "  * Including closed class words improves performance.  One can see from\n",
    "the distinct list of closed class words that are constructed for each\n",
    "sense of \"hard\" that the distributions of closed class wrt word sense\n",
    "are quite distinct and therefore informative.  Furthermore, by\n",
    "including closed class words within the context window one *excludes*\n",
    "open class words that may be, say, 5 or 6 words away from the target\n",
    "word and are hence less informative clues for the target word sense.\n",
    "\n",
    "To see if the data really is too sparse for consistent results, try a different seed for the random number generator, by\n",
    "editting line 211 in the definition of `wst_classifier` to use the seed value from the comment instead of the one it's been using.  Then try again and see how, if at all, the trend as number increases is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [100, 200, 300, 400]:\n",
    "    for stopwords in [[], STOPWORDS]:\n",
    "        stop = 'stopwords' if stopwords else 'no stopwords'\n",
    "        print('Word features with number: %s and %s'%(n, stop))\n",
    "        wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features, number=n, stopwords_list=stopwords) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems slightly odd that the word features for 'hard.pos' include _harder_ and _hardest_. Try using a stopwords list which adds them to STOPWORDS: is the effect what you expected? Can you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features, number=300, stopwords_list=STOPWORDS)\n",
    "wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_word_features, number=300, stopwords_list=STOPWORDS+['harder', 'hardest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy goes down. This might be expected if a particular word sense would be more likely to appear together with harder and hardest. This means that removing the two words would remove relevant information which would be replaced by some very infrequent words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis\n",
    "The function `wst_classifier` allows you to explore the errors of the model it creates:\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "You can output a confusion matrix as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wst_classifier(NaiveBayesClassifier.train, 'hard.pos',\n",
    "               wsd_context_features, distance=3, confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that the rows in the matrix are the gold labels, and the columns are the estimated labels. Recall that the diagonal line represents the number of items that the model gets right. \n",
    "#### Errors\n",
    "\n",
    "You can also output each error from the test data into a file `errors.txt`. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wst_classifier(NaiveBayesClassifier.train, 'hard.pos',\n",
    "               wsd_context_features, distance=2, confusion_matrix=True, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your favourite editor to look at `errors.txt`.\n",
    "You will find it in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `errors.txt`, the example number on the first line of each entry is the (list) index of the error in the test_data. \n",
    "\n",
    "#### Now it's your turn:\n",
    "\n",
    "1. Choose your best performing model from your earlier trials, and train it again, but add the arguments `confusion_matrix=True` and `log=True`.\n",
    "2. Using the confusion matrix, identify which sense is the hardest one for the model to estimate.\n",
    "3. Look in `errors.txt` for examples where that hardest word sense is the correct label. Do you see any patterns or systematic errors? If so, can you think of a way to adapt the feature vector so as to improve the model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wst_classifier(NaiveBayesClassifier.train, 'hard.pos', wsd_context_features,  distance=1, confusion_matrix=True, log=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((14+8)/(680+14+8))\n",
    "print((20+3)/(20+65+3))\n",
    "print((17+7)/(17+7+53))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HARD3 is the most difficult sense for the classifier. There isn't one right answer for this question. It is more of a question to invite speculation and let you think about your classifier. The most obvious pattern is that HARD1 is extremely dominant in the number of examples. This can be seen in the classification results as the majority of error comes from HARD2 and HARD3 being missclassified as HARD1. It should be noted that the classifier used only looks at words at a distance of 1. This really isn't very much context. Due to data sparsity I imagine that a lot of error simply comes from the fact that a particular context may not have been seen before. For example, _HARD shoulders_ and _HARD soled shoes_ seem like obvious examples of HARD3 but they have been classed as HARD1. Most likely these things were simply not found in the dataset. One thing that seems to happen quite a few times is that the HARD ends up next to an adverb or a different adjective, such as _slightly HARDER_ which could definitely appear next to any sense of the word. What might be useful is to always include information about the POS context in which the word appears or parsing information. HARD3 will generally attach to nouns, as in _hard seats_ or _hard hats_. While HARD1 has more of a spread. \n",
    "\n",
    "Again, there isn't one correct answer, see what you can spot and try and come up with some reasonable suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
