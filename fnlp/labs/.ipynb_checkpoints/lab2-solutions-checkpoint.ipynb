{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNLP: Lab Session 2\n",
    "\n",
    "### Smoothing and Authorship Identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries used for this lab\n",
    "\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "# Import the gutenberg corpus\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# Import probability distributions\n",
    "from nltk.probability import LaplaceProbDist\n",
    "from nltk.probability import LidstoneProbDist\n",
    "from nltk.probability import SimpleGoodTuringProbDist\n",
    "\n",
    "# The NgramModel from NLTK version 2 has been removed from NLTK 3.\n",
    "# So we're using a ported version from a local directory.\n",
    "try:\n",
    "    from nltk_model import *  # See the README inside the nltk_model folder for more information\n",
    "except ImportError:\n",
    "    from .nltk_model import *  # Compatibility depending on how this script was run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "The aims of this lab session are to explore 1) the Laplace, Lidstone and Good-\n",
    "Turing smoothing methods for language models and 2) the use of language\n",
    "models in authorship identification. Successful completion of this lab will help\n",
    "you solidify your understanding of smoothing (important not just for LMs but\n",
    "all over NLP), perplexity (important also for assignment 1), and one type of\n",
    "text classification (authorship identification). By the end of this lab session,\n",
    "you should be able to:\n",
    "\n",
    "    * Compute smoothed bigram probabilities by hand for simple smoothing methods.\n",
    "    * Train an nltk language model with smoothing for unseen n-grams\n",
    "    * Make use of language models to identify the author of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In some of the exercises, you’ll use NLTK’s `NgramModel` to train language models. \n",
    "\n",
    "The initialisation method for NgramModel is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, n, train, pad_left=False, pad_right=False,\n",
    "             estimator=None, *estimator_args, **estimator_kwargs):\n",
    "    \"\"\"\n",
    "    Creates an ngram language model to capture patterns in n consecutive\n",
    "    words of training text.  An estimator smooths the probabilities derived\n",
    "    from the text and may allow generation of ngrams not seen during\n",
    "    training.\n",
    "\n",
    "    :param n: the order of the language model (ngram size)\n",
    "    :type n: C{int}\n",
    "    :param train: the training text\n",
    "    :type train: C{iterable} of C{string} or C{iterable} of C{iterable} of C{string} \n",
    "    :param estimator: a function for generating a probability distribution---defaults to MLEProbDist\n",
    "    :type estimator: a function that takes a C{ConditionalFreqDist} and\n",
    "          returns a C{ConditionalProbDist}\n",
    "    :param pad_left: whether to pad the left of each sentence with an (n-1)-gram of <s>\n",
    "    :type pad_left: bool\n",
    "    :param pad_right: whether to pad the right of each sentence with </s>\n",
    "    :type pad_right: bool\n",
    "    :param estimator_args: Extra arguments for estimator.\n",
    "        These arguments are usually used to specify extra\n",
    "        properties for the probability distributions of individual\n",
    "        conditions, such as the number of bins they contain.\n",
    "        Note: For backward-compatibility, if no arguments are specified, the\n",
    "        number of bins in the underlying ConditionalFreqDist are passed to\n",
    "        the estimator as an argument.\n",
    "    :type estimator_args: (any)\n",
    "    :param estimator_kwargs: Extra keyword arguments for the estimator\n",
    "    :type estimator_kwargs: (any)\n",
    "    \"\"\"\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing\n",
    "\n",
    "In the final exercise of Lab 1, you were asked to calculate the probability of\n",
    "a word given its context, using a bigram language model with no smoothing.\n",
    "For the first two word-context pairs, these bigrams had been seen in the data\n",
    "used to train the language model. For the third word-context pair, the bigram\n",
    "had not been seen in the training data, which led to an estimated probability\n",
    "of 0.0.\n",
    "\n",
    "Zero probabilities for unseen n-grams cause problems. Suppose for example you\n",
    "take a bigram language model and use it to score an automatically generated\n",
    "sentence of 10 tokens (say the output of a machine translation system). If one\n",
    "of the bigrams in that sentence is unseen, the probability of the sentence will\n",
    "be zero.\n",
    "\n",
    "Smoothing is a method of assigning probabilities to unseen n-grams. As language models are typically trained using large amounts of data, any n-gram not\n",
    "seen in the training data is probably unlikely to be seen in other (test) data. A\n",
    "good smoothing method is therefore one that assigns a fairly small probability\n",
    "to unseen n-grams.\n",
    "\n",
    "We’ll implement two different smoothing methods: Laplace (add-one) and Lidstone (add-alpha), and we will also consider the effects of backoff, which is\n",
    "implemented in NLTK’s NgramModel.\n",
    "\n",
    "(NLTK also includes implementations of Laplace and Lidstone smoothing in its\n",
    "probability module; if you wish to look at the implementations they are here:\n",
    "http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "However the point of this lab is to implement these simple methods yourself to\n",
    "make sure you understand them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum-Likelihood estimation\n",
    "\n",
    "Before implementing any smoothing, you should make sure you understand how\n",
    "to implement maximum likelihood estimation. In last week’s lab, we used NLTK\n",
    "to do this for us by training a bigram language model with an MLE estimator.\n",
    "We could then use the language model to find the MLE probability of any word\n",
    "given its context. Here, you’ll do the same thing but without using NLTK,\n",
    "just to make sure you understand how. We will also compare the smoothed\n",
    "probabilities you compute later to these MLE probabilities.\n",
    "\n",
    "#### Exercise 0\n",
    "The code block below extracts all the words from Jane Austen’s “Sense\n",
    "and Sensibility”, and then computes a list of bigram tuples by pairing up each\n",
    "word in the corpus with the following word. Using the resulting lists of unigrams and bigrams,\n",
    "complete the code block so it returns the MLE probability of a word given a\n",
    "single word of context. Then run the test code in the next block to compute the proba-\n",
    "bilities:\n",
    "    \n",
    "1. $ P_{M LE}(“end”|“the”) $\n",
    "2. $ P_{M LE}(“the”|“end”) $\n",
    "\n",
    "Make sure your answers match the MLE probability estimates from Exercise 5\n",
    "of Lab 1, where we used NLTK to compute these estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### EXERCISE 0 ####################\n",
    "\n",
    "# Solution for exercise 0\n",
    "def ex0(word,context):\n",
    "    \"\"\"Estimate the unsmoothed (MLE) probability for word given the single word context,\n",
    "    based on counts from Austen's _Sense and Sensibility_\n",
    "    \n",
    "    :param word: The input word\n",
    "    :type word: str\n",
    "    :param context: The preceding word\n",
    "    :type context: str\n",
    "    :return: The MLE probability of word given context\n",
    "    :rtype: float\"\"\"\n",
    "    p = 0.0\n",
    "\n",
    "    austen_words = [w.lower() for w in gutenberg.words('austen-sense.txt')]\n",
    "    # Bigrams that have occurred (w_i, w_(i+1))\n",
    "    austen_bigrams = list(zip(austen_words[:-1], austen_words[1:]))  # list of bigrams as tuples\n",
    "    # (above doesn't include begin/end of corpus: but basically this is fine)\n",
    "\n",
    "    # Compute probability of word given context\n",
    "    p = float(austen_bigrams.count((context, word))) / austen_words.count(context)\n",
    "\n",
    "    # Return probability\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE:\n",
      "Probability of 'end' given 'the': 0.00584652862362972\n",
      "Probability of 'the' given 'end': 0.0\n"
     ]
    }
   ],
   "source": [
    "### Test your code with these examples\n",
    "print('MLE:')\n",
    "result0a = ex0('end','the')\n",
    "print(\"Probability of 'end' given 'the': %s\"%result0a)\n",
    "result0b = ex0('the','end')\n",
    "print(\"Probability of 'the' given 'end': %s\"%result0b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace (add-1)\n",
    "\n",
    "Laplace smoothing adds a value of 1 to the sample count for each “bin” (possible\n",
    "observation, in this case each possible bigram), and then takes the maximum\n",
    "likelihood estimate of the resulting frequency distribution.\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Assume that the size of the vocabulary is just the number of different words\n",
    "observed in the training data (that is, we will not deal with unseen words).\n",
    "Add code to the template to compute Laplace smoothed probabilities, again\n",
    "without using NLTK.\n",
    "Hint: if you have trouble, study the equations and example in Lecture 4\n",
    "Now uncomment the test code and look at the estimates for:\n",
    "\n",
    "1. $P_{+1} (“end”|“the”)$\n",
    "2. $P_{+1} (“the”|“end”)$\n",
    "\n",
    "How do these probabilities differ from the MLE estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### EXERCISE 1 ####################\n",
    "\n",
    "# Solution for exercise 1\n",
    "def ex1(word,context):\n",
    "    \"\"\"Estimate the Laplace smoothed probability for word given the single word context,\n",
    "    based on counts from Austen's _Sense and Sensibility_\n",
    "    \n",
    "    :param word: The input word\n",
    "    :type word: str\n",
    "    :param context: The preceding word\n",
    "    :type context: str\n",
    "    :return: The Laplace-smoothed probability of word given context\n",
    "    :rtype: float\"\"\"\n",
    "    p = 0.0\n",
    "\n",
    "    austen_words = [w.lower() for w in gutenberg.words('austen-sense.txt')]\n",
    "    austen_bigrams = list(zip(austen_words[:-1], austen_words[1:]))  # list of bigrams as tuples\n",
    "    # (above doesn't include begin/end of corpus: but basically this is fine)\n",
    "    V = len(set(austen_words)) # vocabulary size\n",
    "\n",
    "    # Compute probability of word given context\n",
    "    p = float(austen_bigrams.count((context, word))+1) / (austen_words.count(context)+V)\n",
    "\n",
    "    # Return probability\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAPLACE:\n",
      "Probability of 'end' given 'the': 0.002379139703083365\n",
      "Probability of 'the' given 'end': 0.0001548467017652524\n"
     ]
    }
   ],
   "source": [
    "### Test your code with these examples\n",
    "print('LAPLACE:')\n",
    "result1a = ex1('end','the')\n",
    "print(\"Probability of 'end' given 'the': %s\"%result1a)\n",
    "result1b = ex1('the','end')\n",
    "print(\"Probability of 'the' given 'end': %s\"%result1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidstone (add-alpha)\n",
    "\n",
    "In practice, Laplace smoothing assigns too much mass to unseen n-grams. The\n",
    "Lidstone method works in a similar way, but instead of adding 1, it adds a value\n",
    "between 0 and 1 to the sample count for each bin (in class we called this value\n",
    "alpha, NLTK calls it gamma).\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Fill in the code to compute Lidstone smoothed probabilities, then try\n",
    "the test code and look at the probability estimates that are computed for the\n",
    "same bigrams as before using various values of alpha.\n",
    "\n",
    "What do you notice about using `alpha = 0` and `alpha = 1`? (Compare to the\n",
    "probabilities computed by the previous methods.)\n",
    "\n",
    "What about when `alpha = 0.01`? Are the estimated probabilities more similar\n",
    "to MLE or Laplace smoothing in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### EXERCISE 2 ####################\n",
    "\n",
    "# Solution for exercise 2\n",
    "def ex2(word,context,alpha):\n",
    "    \"\"\"Estimate the probability, Lidstone-smoothed by alpha, for word given the single word context,\n",
    "    based on counts from Austen's _Sense and Sensibility_\n",
    "    \n",
    "    :param word: The input word\n",
    "    :type word: str\n",
    "    :param context: The preceding word\n",
    "    :type context: str\n",
    "    :param alpha: The (fractional) count to add for each word type\n",
    "    :type alpha: float\n",
    "    :return: The Lidstone-smoothed probability of word given context with parameter alpha\n",
    "    :rtype: float\"\"\"\n",
    "    p =0.0\n",
    "\n",
    "    austen_words = [w.lower() for w in gutenberg.words('austen-sense.txt')]\n",
    "    austen_bigrams = list(zip(austen_words[:-1], austen_words[1:]))  # list of bigrams as tuples\n",
    "    # (above doesn't include begin/end of corpus: but basically this is fine)\n",
    "    V = len(set(austen_words)) # vocabulary size\n",
    "    p = float(austen_bigrams.count((context, word))+alpha) / (austen_words.count(context)+V*alpha)\n",
    "\n",
    "    # Return probability\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIDSTONE, alpha=0.01:\n",
      "Probability of 'end' given 'the': 0.005759133419524446\n",
      "Probability of 'the' given 'end': 8.401243384020835e-05\n",
      "LIDSTONE, alpha=0:\n",
      "Probability of 'end' given 'the': 0.00584652862362972\n",
      "Probability of 'the' given 'end': 0.0\n",
      "LIDSTONE, alpha=1:\n",
      "Probability of 'end' given 'the': 0.002379139703083365\n",
      "Probability of 'the' given 'end': 0.0001548467017652524\n"
     ]
    }
   ],
   "source": [
    "### Test your code with these examples\n",
    "print('LIDSTONE, alpha=0.01:')\n",
    "result2a = ex2('end','the',.01)\n",
    "print(\"Probability of 'end' given 'the': %s\"%result2a)\n",
    "result2b = ex2('the','end',.01)\n",
    "print(\"Probability of 'the' given 'end': %s\"%result2b)\n",
    "print(\"LIDSTONE, alpha=0:\")\n",
    "result2c = ex2('end','the',0)\n",
    "print(\"Probability of 'end' given 'the': %s\"%result2c)\n",
    "result2d = ex2('the','end',0)\n",
    "print(\"Probability of 'the' given 'end': %s\"%result2d)\n",
    "print(\"LIDSTONE, alpha=1:\")\n",
    "result2e = ex2('end','the',1)\n",
    "print(\"Probability of 'end' given 'the': %s\"%result2e)\n",
    "result2f = ex2('the','end',1)\n",
    "print(\"Probability of 'the' given 'end': %s\"%result2f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff\n",
    "\n",
    "Now we will look at the effects of incorporating backoff in addition to some of\n",
    "these simple smoothing methods. In a bigram language model with backoff, the\n",
    "probability of an unseen bigram is computed by “backing off”: that is, if a word\n",
    "has never been seen in a particular context, then we compute its probability by\n",
    "using one fewer context words. Backing off from a bigram model (one word of\n",
    "context) therefore means we’d get estimates based on unigram frequencies (no\n",
    "context).\n",
    "\n",
    "The mathematical details of backoff are a bit complex to ensure all the probabilities\n",
    "sum to 1. You needn’t understand all the details of backoff but you\n",
    "should understand these basic principles:\n",
    "\n",
    "    * Bigram probabilities for seen bigrams will be slightly lower than MLE in order to allocate some probability mass to unseen bigrams.\n",
    "    * The unigram probabilities inside the backoff (i.e. the ones we use if we didn’t see the bigram) are similar in their relatives sizes to the unigram probabilities we would get if we just estimated a unigram model directly.\n",
    "\n",
    "That is, a word with high corpus frequency will have a higher unigram\n",
    "backoff probability than a word with a low corpus frequency.\n",
    "Look back at the initialization method for NgramModel earlier in the lab. If\n",
    "you pass in MLEProbDist as the estimator (which we did in the last lab), then\n",
    "no backoff is used. However, with any other estimator (i.e., smoothing), the\n",
    "NgramModel does use backoff.\n",
    "\n",
    "#### Exercise 3\n",
    "\n",
    "Uncomment the code in the block below to train a bigram language model using Jane\n",
    "Austen’s “Sense and Sensibility” and Laplace smoothing. (By using Laplace\n",
    "as the estimator, you are also turning on backoff in NgramModel.) Use this\n",
    "language model to compute the probability of the same bigrams we’ve been\n",
    "looking at all along. Then try the test code to see the results.\n",
    "\n",
    "    * Compare the probabilities you get for these bigrams to what you got when you computed Laplace yourself. Why are the probabilities produced by NgramModel with Laplace smoothing different from the probabilities you computed yourself?\n",
    "    * Now look at the estimated probabilities P'(“end”|“the”) and P'(“the”|“end”) as computed by the NgramModel and by the previous smoothing methods. Which method(s) produce larger differences between those probabilities? Do all the methods agree about which bigram has higher probability? If not, what is the reason for the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### EXERCISE 3 ####################\n",
    "\n",
    "# Solution for exercise 3\n",
    "def ex3(word,context):\n",
    "    \"\"\"Estimate the Laplace smoothed probability for word given the single word context, with backoff,\n",
    "    based on counts from Austen's _Sense and Sensibility_\n",
    "    \n",
    "    :param word: The input word\n",
    "    :type word: str\n",
    "    :param context: The preceding word\n",
    "    :type context: str\"\"\"\n",
    "    p =0.0\n",
    "\n",
    "    austen_words = [w.lower() for w in gutenberg.words('austen-sense.txt')]\n",
    "\n",
    "    # Train a bigram language model using a LAPLACE estimator AND BACKOFF\n",
    "    lm = NgramModel(2,austen_words,estimator=lambda f,b: LaplaceProbDist(f,b+1))\n",
    "    # Compute probability of word given context (note lm requires a list context)\n",
    "    p = lm.prob(word,[context])\n",
    "\n",
    "    # Return probability\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKOFF WITH LAPLACE\n",
      "Probability of 'end' given 'the': 0.002378913312398896\n",
      "Probability of 'the' given 'end': 0.03394548658640203\n"
     ]
    }
   ],
   "source": [
    "### Test your code with these examples\n",
    "print('BACKOFF WITH LAPLACE')\n",
    "result3a = ex3('end','the')\n",
    "print(\"Probability of 'end' given 'the': %s\"%result3a)\n",
    "result3b = ex3('the','end')\n",
    "print(\"Probability of 'the' given 'end': %s\"%result3b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship Identification\n",
    "\n",
    "## Cross-entropy\n",
    "\n",
    "In language modelling, a model is trained on a set of data (i.e. the training\n",
    "data). The cross-entropy of this model may then be measured on a test set\n",
    "(i.e. another set of data that is different from the training data) to assess how\n",
    "accurate the model is in predicting the test data.\n",
    "\n",
    "Another way to look at this is: if we used the trained model to generate new\n",
    "sentences by sampling words from its probability distribution, how similar would\n",
    "those new sentences be to the sentences in the test data? This interpretation\n",
    "allows us to use cross-entropy for authorship detection, as described below.\n",
    "\n",
    "`NgramModel` contains the following cross-entropy method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(self, text, pad_left=False, pad_right=False,\n",
    "    verbose=False, perItem=False):\n",
    "    \"\"\"\n",
    "    Calculate the approximate cross-entropy of the n-gram model for a\n",
    "    given evaluation text.\n",
    "    This is the average log probability of each item in the text.\n",
    "    :param text: items to use for evaluation\n",
    "    :type text: iterable(str)\n",
    "    :param pad_left: whether to pad the left of each text with an (n-1)-gram\\\n",
    "    of <s> markers\n",
    "    :type pad_left: bool\n",
    "    :param pad_right: whether to pad the right of each sentence with an </s>\\\n",
    "    marker\n",
    "    :type pad_right: bool\n",
    "    :param perItem: normalise for length if True\n",
    "    :type perItem: bool\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "We can use cross-entropy in authorship detection. For example, suppose we have\n",
    "a language model trained on Jane Austen’s “Sense and Sensibility” (training\n",
    "data) plus the texts for two other novels (test data), one by Jane Austen and\n",
    "one by another author, but we don’t know which is which. We can work out the\n",
    "cross-entropy of the model on each of the texts and from the scores, determine\n",
    "which of the two test texts was more likely written by Jane Austen.\n",
    "Use:\n",
    "\n",
    "* A trigram language model with a Lidstone probability distribution, trained on Jane Austen’s “Sense and Sensibility” (austen-sense.txt) N.B. The “f.B()+1” argument (already provided for you in the code) means that we lump together all the unseen n-grams as a single “unknown” token.\n",
    "* text a: austen-emma.txt (Jane Austen’s “Emma”)\n",
    "* text b: chesterton-ball.txt (G.K. Chesterton’s “The Ball and Cross”)\n",
    "* `NgramModel`’s entropy function: `lm.entropy(...)` \n",
    "    \n",
    "Compute both the total cross-entropy and the per-word cross entropy of each text. (Separate function templates are provided.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### EXERCISE 4 ####################\n",
    "\n",
    "# Solutions for exercise 4\n",
    "\n",
    "def ex4_tot_entropy(lm,doc_name):\n",
    "    \"\"\"Use a language model to compute the total word-level entropy of a document\n",
    "    \n",
    "    :param lm: a language model\n",
    "    :type lm: ngram.NgramModel\n",
    "    :param doc_name: A gutenberg document name\n",
    "    :type doc_name: str\n",
    "    :return: The total entropy of the named document per the model\n",
    "    :rtype: float\"\"\"\n",
    "    e = 0.0\n",
    "\n",
    "    # Construct a list of lowercase words from the document (test document)\n",
    "    doc_words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "\n",
    "    # Compute the total cross entropy of the text in doc_name\n",
    "    e = lm.entropy(doc_words, perItem=False)\n",
    "\n",
    "    # Return the entropy\n",
    "    return e\n",
    "\n",
    "def ex4_perword_entropy(lm,doc_name):\n",
    "    \"\"\"Use a language model to compute the average (per-word) word-level entropy of a document\n",
    "    \n",
    "    :param lm: a language model\n",
    "    :type lm: ngram.NgramModel\n",
    "    :param doc_name: A gutenberg document name\n",
    "    :type doc_name: str\n",
    "    :return: The per-word entropy of the named document per the model\n",
    "    :rtype: float\"\"\"\n",
    "    e = 0.0\n",
    "\n",
    "    # Construct a list of lowercase words from the document (test document)\n",
    "    doc_words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "\n",
    "    # Compute the normalized cross entropy of the text in doc_name\n",
    "    e = lm.entropy(doc_words, perItem=True)\n",
    "\n",
    "    # Return the entropy\n",
    "    return e\n",
    "\n",
    "\n",
    "def ex4_lm(doc_name):\n",
    "    \"\"\"Compute a word-level language model from a document\n",
    "    \n",
    "    :param doc_name: A gutenberg document name\n",
    "    :type doc_name: str\n",
    "    :return: a language model\n",
    "    :rtype: ngram.NgramModel\"\"\"\n",
    "    l = None\n",
    "\n",
    "    # Construct a list of lowercase words from the document (training data for lm)\n",
    "    doc_words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "\n",
    "    # Train a trigram language model with backoff using doc_words and    \n",
    "    # a Lidstone probability distribution with +0.01 added to the sample count for each bin\n",
    "    l = NgramModel(3,doc_words,estimator=lambda f,b:nltk.LidstoneProbDist(f,0.01,f.B()+1))\n",
    "\n",
    "    # Return the language model\n",
    "    return l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cross-entropy for austen-emma.txt: 2811146.288420416\n",
      "Total cross-entropy for chesterton-ball.txt: 1678081.011543291\n",
      "Per-word cross-entropy for austen-emma.txt: 14.609049179786494\n",
      "Per-word cross-entropy for chesterton-ball.txt: 17.300874399893715\n"
     ]
    }
   ],
   "source": [
    "### Test your code with these examples\n",
    "lm4 = ex4_lm('austen-sense.txt')\n",
    "result4a = ex4_tot_entropy(lm4,'austen-emma.txt')\n",
    "print('Total cross-entropy for austen-emma.txt: %s'%result4a)\n",
    "result4b = ex4_tot_entropy(lm4,'chesterton-ball.txt')\n",
    "print('Total cross-entropy for chesterton-ball.txt: %s'%result4b)\n",
    "result4c = ex4_perword_entropy(lm4,'austen-emma.txt')\n",
    "print('Per-word cross-entropy for austen-emma.txt: %s'%result4c)\n",
    "result4d = ex4_perword_entropy(lm4,'chesterton-ball.txt')\n",
    "print('Per-word cross-entropy for chesterton-ball.txt: %s'%result4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Going further\n",
    "##  Padding\n",
    "\n",
    "Redo exercise 4 setting `pad_left` and `pad_right` to `True` both when initialising\n",
    "the n-gram model and when computing entropy. What difference does this\n",
    "make?\n",
    "\n",
    "## Sentences\n",
    "\n",
    "Using one enormous string of words as the training and test data is less than optimal, as it trains/tests across sentence boundaries.  Look back at the argument description for the `train` argument to `NgramModel` and see that it will actually train on an input which is a list of list of words, that is, a list of *sentences*, padding each sentence appropriately.  Redo exercise 4 training and testing on the sentences in the specified documents.\n",
    "\n",
    "## Case\n",
    "\n",
    "If we're training on sentences, maybe we shouldn't be down-casing?  Give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
