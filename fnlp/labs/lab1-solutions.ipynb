{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FNLP: Lab Session 1\n",
    "### Corpora and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "\n",
    "The aims of this lab session are to 1) explore the different uses of language in\n",
    "different documents, authored by different people and 2) introduce the construction of language models using Python’s Natural Language Tool Kit (NLTK).\n",
    "Successful completion of this lab is important as the first assignment for FNLP\n",
    "builds on some of the concepts and methods that are introduced here. By the\n",
    "end of this lab session, you should be able to:\n",
    "\n",
    "* Access the corpora provided in NLTK\n",
    "* Compute a frequency distribution\n",
    "* Train a language model\n",
    "* Use a language model to compute bigram probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running NLTK, Jupyter, and Python Help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Jupyter and NLTK\n",
    "\n",
    "This year our recommended method for running labs is through Jupyter Notebooks.\n",
    "\n",
    "To run the notebook locally, download it from the website and `cd` into the directory you saved it in. Next, activate the FNLP conda environment by running `conda activate fnlp`. Then run `jupyter notebook`. This will should open a browser window showing the contents of your working directory. Click on lab1.ipynb.\n",
    "\n",
    "Now that you are here you can run the code in any of the cells. The simplest way to do this is by hitting either `ctrl+enter` or `shift+enter` (the former will run the current cell while the latter will run the cell and move the focus to the next cell)\n",
    "\n",
    "Try it out by importing NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Help \n",
    "\n",
    "Python contains an inbuilt help module that runs in an interactive mode. To\n",
    "run the interactive help, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to Python 3.7's help utility!\n",
      "\n",
      "If this is your first time using Python, you should definitely check out\n",
      "the tutorial on the Internet at https://docs.python.org/3.7/tutorial/.\n",
      "\n",
      "Enter the name of any module, keyword, or topic to get help on writing\n",
      "Python programs and using Python modules.  To quit this help utility and\n",
      "return to the interpreter, just type \"quit\".\n",
      "\n",
      "To get a list of available modules, keywords, symbols, or topics, type\n",
      "\"modules\", \"keywords\", \"symbols\", or \"topics\".  Each module also comes\n",
      "with a one-line summary of what it does; to list the modules whose name\n",
      "or summary contain a given string such as \"spam\", type \"modules spam\".\n",
      "\n",
      "\n",
      "You are now leaving help and returning to the Python interpreter.\n",
      "If you want to ask for help on a particular object directly from the\n",
      "interpreter, you can type \"help(object)\".  Executing \"help('string')\"\n",
      "has the same effect as typing a particular string at the help> prompt.\n"
     ]
    }
   ],
   "source": [
    "help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`help()` will run until interrupted. If a cell is running it will block any other cell from running until it has completed. You can check if a cell is still running by looking at `In [*]:` to the left of any cell. If there is a `*` inside the brackets the cell is still running. As soon as the cell has stopped running the `*` will be replaced by a number. \n",
    "\n",
    "**Before moving on** you will need to interupt `help()` (make it stop running). To interupt running cells go to **`kernel/interrupt`** at the top of the webpage. You can also hit the **big black square button** right underneath (if you hover over it it will say interrupt kernel). This is equivalent to hitting CTRL-d to interrupt a running program in the terminal or the python shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the name of the module that you want to get help on, type:\n",
    "`import <module_name>`\n",
    "`help(<module_name>)`\n",
    "try looking at the help documentation for `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk:\n",
      "\n",
      "NAME\n",
      "    nltk\n",
      "\n",
      "DESCRIPTION\n",
      "    The Natural Language Toolkit (NLTK) is an open source Python library\n",
      "    for Natural Language Processing.  A free online book is available.\n",
      "    (If you use the library for academic research, please cite the book.)\n",
      "    \n",
      "    Steven Bird, Ewan Klein, and Edward Loper (2009).\n",
      "    Natural Language Processing with Python.  O'Reilly Media Inc.\n",
      "    http://nltk.org/book\n",
      "    \n",
      "    @version: 3.3\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    app (package)\n",
      "    book\n",
      "    ccg (package)\n",
      "    chat (package)\n",
      "    chunk (package)\n",
      "    classify (package)\n",
      "    cluster (package)\n",
      "    collections\n",
      "    collocations\n",
      "    compat\n",
      "    corpus (package)\n",
      "    data\n",
      "    decorators\n",
      "    downloader\n",
      "    draw (package)\n",
      "    featstruct\n",
      "    grammar\n",
      "    help\n",
      "    inference (package)\n",
      "    internals\n",
      "    jsontags\n",
      "    lazyimport\n",
      "    metrics (package)\n",
      "    misc (package)\n",
      "    parse (package)\n",
      "    probability\n",
      "    sem (package)\n",
      "    sentiment (package)\n",
      "    stem (package)\n",
      "    tag (package)\n",
      "    tbl (package)\n",
      "    test (package)\n",
      "    text\n",
      "    tgrep\n",
      "    tokenize (package)\n",
      "    toolbox\n",
      "    translate (package)\n",
      "    tree\n",
      "    treeprettyprinter\n",
      "    treetransforms\n",
      "    twitter (package)\n",
      "    util\n",
      "    wsd\n",
      "\n",
      "SUBMODULES\n",
      "    agreement\n",
      "    aline\n",
      "    api\n",
      "    association\n",
      "    bleu_score\n",
      "    bllip\n",
      "    boxer\n",
      "    brill\n",
      "    brill_trainer\n",
      "    casual\n",
      "    chart\n",
      "    confusionmatrix\n",
      "    corenlp\n",
      "    crf\n",
      "    decisiontree\n",
      "    dependencygraph\n",
      "    discourse\n",
      "    distance\n",
      "    drt\n",
      "    earleychart\n",
      "    evaluate\n",
      "    featurechart\n",
      "    glue\n",
      "    hmm\n",
      "    hunpos\n",
      "    ibm1\n",
      "    ibm2\n",
      "    ibm3\n",
      "    ibm4\n",
      "    ibm5\n",
      "    ibm_model\n",
      "    isri\n",
      "    lancaster\n",
      "    lfg\n",
      "    linearlogic\n",
      "    logic\n",
      "    mace\n",
      "    malt\n",
      "    mapping\n",
      "    maxent\n",
      "    megam\n",
      "    mwe\n",
      "    naivebayes\n",
      "    nonprojectivedependencyparser\n",
      "    paice\n",
      "    pchart\n",
      "    perceptron\n",
      "    porter\n",
      "    positivenaivebayes\n",
      "    projectivedependencyparser\n",
      "    prover9\n",
      "    punkt\n",
      "    recursivedescent\n",
      "    regexp\n",
      "    relextract\n",
      "    repp\n",
      "    resolution\n",
      "    ribes_score\n",
      "    rslp\n",
      "    rte_classify\n",
      "    scikitlearn\n",
      "    scores\n",
      "    segmentation\n",
      "    senna\n",
      "    sequential\n",
      "    sexpr\n",
      "    shiftreduce\n",
      "    simple\n",
      "    snowball\n",
      "    spearman\n",
      "    stack_decoder\n",
      "    stanford\n",
      "    stanford_segmenter\n",
      "    tableau\n",
      "    tadm\n",
      "    textcat\n",
      "    texttiling\n",
      "    tnt\n",
      "    toktok\n",
      "    transitionparser\n",
      "    treebank\n",
      "    viterbi\n",
      "    weka\n",
      "    wordnet\n",
      "\n",
      "FUNCTIONS\n",
      "    demo()\n",
      "        # override any accidentally imported demo\n",
      "\n",
      "DATA\n",
      "    RUS_PICKLE = 'taggers/averaged_perceptron_tagger_ru/averaged_perceptro...\n",
      "    SLASH = *slash*\n",
      "    TYPE = *type*\n",
      "    __author_email__ = 'stevenbird1@gmail.com'\n",
      "    __classifiers__ = ['Development Status :: 5 - Production/Stable', 'Int...\n",
      "    __copyright__ = 'Copyright (C) 2001-2018 NLTK Project.\\n\\nDistribut......\n",
      "    __keywords__ = ['NLP', 'CL', 'natural language processing', 'computati...\n",
      "    __license__ = 'Apache License, Version 2.0'\n",
      "    __longdescr__ = 'The Natural Language Toolkit (NLTK) is a Python ... p...\n",
      "    __maintainer__ = 'Steven Bird, Edward Loper, Ewan Klein'\n",
      "    __maintainer_email__ = 'stevenbird1@gmail.com'\n",
      "    __url__ = 'http://nltk.org/'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    app = <LazyModule 'nltk.nltk.app'>\n",
      "    chat = <LazyModule 'nltk.nltk.chat'>\n",
      "    class_types = (<class 'type'>,)\n",
      "    corpus = <LazyModule 'nltk.nltk.corpus'>\n",
      "    improved_close_quote_regex = re.compile('([»”’])')\n",
      "    improved_open_quote_regex = re.compile('([«“‘„]|[`]+)')\n",
      "    improved_punct_regex = re.compile('([^\\\\.])(\\\\.)([\\\\]\\\\)}>\"\\\\\\'»”’ ]*)...\n",
      "    infile = <_io.TextIOWrapper name='/group/teaching/conda/e...packages/n...\n",
      "    json_tags = {'!nltk.tag.BrillTagger': <class 'nltk.tag.brill.BrillTagg...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    string_types = (<class 'str'>,)\n",
      "    toolbox = <LazyModule 'nltk.nltk.toolbox'>\n",
      "    version_file = '/group/teaching/conda/envs/fnlp/lib/python3.7/site-pac...\n",
      "    version_info = sys.version_info(major=3, minor=7, micro=1, releaseleve...\n",
      "\n",
      "VERSION\n",
      "    3.3\n",
      "\n",
      "AUTHOR\n",
      "    Steven Bird, Edward Loper, Ewan Klein\n",
      "\n",
      "FILE\n",
      "    /group/teaching/conda/envs/fnlp/lib/python3.7/site-packages/nltk/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the name of the module and the method that you want to get help\n",
    "on, type `help(<module_name>.<method_name>)` (note you must have imported `<module_name>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The FNLP lab sessions will make use of the Natural Language Tool Kit (NLTK)\n",
    "for Python. NLTK is a platform for writing programs to process human language\n",
    "data, that provides both corpora and modules. For more information on NLTK,\n",
    "please visit: http://www.nltk.org/.\n",
    "\n",
    "For each exercise, edit the corresponding function in the notebook (e.g. ex1\n",
    "for Exercise 1), then run the lines which prepare for and invoke that\n",
    "function.\n",
    "\n",
    "If you’re unfamiliar with developing python code, you may want to look at the\n",
    "second lab for ANLP, which assumes much less background experience and has\n",
    "a detailed step-by-step guide to using python for the first time:\n",
    "\n",
    "http://www.inf.ed.ac.uk/teaching/courses/anlp/labs/lab2.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Corpora\n",
    "\n",
    "NLTK provides many corpora and covers many genres of text. Some of the\n",
    "corpora are listed below:\n",
    "\n",
    "* Gutenberg: out of copyright books\n",
    "* Brown: a general corpus of texts including novels, short stories and news\n",
    "articles\n",
    "* Inaugural: U.S. Presidential inaugural speeches\n",
    "\n",
    "To see a complete list of available corpora you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'alpino', 'biocreative_ppi', 'brown', 'brown_tei', 'cess_cat', 'cess_esp', 'chat80', 'city_database', 'cmudict', 'comparative_sentences', 'comtrans', 'conll2000', 'conll2002', 'conll2007', 'crubadan', 'dependency_treebank', 'dolch', 'europarl_raw', 'floresta', 'framenet_v15', 'framenet_v17', 'gazetteers', 'genesis', 'gutenberg', 'ieer', 'inaugural', 'indian', 'jeita', 'kimmo', 'knbc', 'lin_thesaurus', 'mac_morpho', 'machado', 'masc_tagged', 'movie_reviews', 'mte_teip5', 'names', 'nombank.1.0', 'nonbreaking_prefixes', 'nps_chat', 'omw', 'opinion_lexicon', 'panlex_swadesh', 'paradigms', 'pe08', 'pil', 'pl196x', 'ppattach', 'problem_reports', 'product_reviews_1', 'product_reviews_2', 'propbank', 'pros_cons', 'ptb', 'ptb3', 'qc', 'reuters', 'rte', 'semcor', 'senseval', 'sentence_polarity', 'sentiwordnet', 'shakespeare', 'sinica_treebank', 'smultron', 'state_union', 'stopwords', 'subjectivity', 'swadesh', 'switchboard', 'timit', 'toolbox', 'treebank', 'twitter_samples', 'udhr', 'udhr2', 'unicode_samples', 'universal_treebanks_v20', 'verbnet', 'webtext', 'wordnet', 'wordnet_ic', 'words', 'ycoe', 'abc.xml', 'alpino.xml', 'biocreative_ppi.xml', 'brown.xml', 'brown_tei.xml', 'cess_cat.xml', 'cess_esp.xml', 'chat80.xml', 'city_database.xml', 'cmudict.xml', 'comparative_sentences.xml', 'comtrans.xml', 'conll2000.xml', 'conll2002.xml', 'conll2007.xml', 'crubadan.xml', 'dolch.xml', 'dependency_treebank.xml', 'europarl_raw.xml', 'floresta.xml', 'framenet_v15.xml', 'framenet_v17.xml', 'gazetteers.xml', 'genesis.xml', 'gutenberg.xml', 'ieer.xml', 'inaugural.xml', 'indian.xml', 'jeita.xml', 'kimmo.xml', 'knbc.xml', 'lin_thesaurus.xml', 'listing.csv', 'mac_morpho.xml', 'machado.xml', 'masc_tagged.xml', 'movie_reviews.xml', 'mte_teip5.xml', 'names.xml', 'nombank.1.0.xml', 'nonbreaking_prefixes.xml', 'nps_chat.xml', 'omw.xml', 'opinion_lexicon.xml', 'panlex_swadesh.xml', 'paradigms.xml', 'pe08.xml', 'pil.xml', 'pl196x.xml', 'ppattach.xml', 'problem_reports.xml', 'product_reviews_1.xml', 'product_reviews_2.xml', 'propbank.xml', 'pros_cons.xml', 'ptb.xml', 'qc.xml', 'reuters.xml', 'rte.xml', 'semcor.xml', 'senseval.xml', 'sentence_polarity.xml', 'sentiwordnet.xml', 'shakespeare.xml', 'sinica_treebank.xml', 'smultron.xml', 'state_union.xml', 'stopwords.xml', 'subjectivity.xml', 'swadesh.xml', 'switchboard.xml', 'timit.xml', 'toolbox.xml', 'treebank.xml', 'twitter_samples.xml', 'udhr.xml', 'udhr2.xml', 'unicode.notes', 'unicode_samples.xml', 'universal_treebanks_v20.xml', 'verbnet.xml', 'webtext.xml', 'wordnet.xml', 'wordnet_ic.xml', 'words.xml', 'ycoe.xml', 'knbc.zip', 'omw.zip', 'stopwords.zip', 'nonbreaking_prefixes.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(nltk.data.find('corpora')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each corpus contains a number of texts. We’ll work with the inaugural corpus,\n",
    "and explore what the corpus contains. Make sure you have imported the nltk\n",
    "module first and then load the inaugural corpus by typing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To list all of the documents in the inaugural corpus, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on we’ll work with President Barack Obama’s inaugural speech\n",
    "from 2009 (2009-Obama.txt). The contents of each document (in a corpus) may\n",
    "be accessed via a number of corpus readers. The plaintext corpus reader provides\n",
    "methods to view the raw text (raw), a list of words (words) or a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My fellow citizens:\n",
      "\n",
      "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition.\n",
      "\n",
      "Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\n",
      "\n",
      "So it has been. So it must be with this generation of Americans.\n",
      "\n",
      "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
      "\n",
      "These are the indicators of crisis, subject to data and statistics. Less measurable but no less profound is a sapping of confidence across our land -- a nagging fear that America's decline is inevitable, that the next generation must lower its sights.\n",
      "\n",
      "Today I say to you that the challenges we face are real. They are serious and they are many. They will not be met easily or in a short span of time. But know this, America -- they will be met.\n",
      "\n",
      "On this day, we gather because we have chosen hope over fear, unity of purpose over conflict and discord.\n",
      "\n",
      "On this day, we come to proclaim an end to the petty grievances and false promises, the recriminations and worn-out dogmas that for far too long have strangled our politics.\n",
      "\n",
      "We remain a young nation, but in the words of Scripture, the time has come to set aside childish things. The time has come to reaffirm our enduring spirit; to choose our better history; to carry forward that precious gift, that noble idea, passed on from generation to generation: the God-given promise that all are equal, all are free, and all deserve a chance to pursue their full measure of happiness.\n",
      "\n",
      "In reaffirming the greatness of our nation, we understand that greatness is never a given. It must be earned. Our journey has never been one of shortcuts or settling for less. It has not been the path for the faint-hearted -- for those who prefer leisure over work, or seek only the pleasures of riches and fame. Rather, it has been the risk-takers, the doers, the makers of things'some celebrated but more often men and women obscure in their labor, who have carried us up the long, rugged path towards prosperity and freedom.\n",
      "\n",
      "For us, they packed up their few worldly possessions and traveled across oceans in search of a new life.\n",
      "\n",
      "For us, they toiled in sweatshops and settled the West; endured the lash of the whip and plowed the hard earth.\n",
      "\n",
      "For us, they fought and died, in places like Concord and Gettysburg; Normandy and Khe Sahn.\n",
      "\n",
      "Time and again these men and women struggled and sacrificed and worked till their hands were raw so that we might live a better life. They saw America as bigger than the sum of our individual ambitions; greater than all the differences of birth or wealth or faction.\n",
      "\n",
      "This is the journey we continue today. We remain the most prosperous, powerful nation on Earth. Our workers are no less productive than when this crisis began. Our minds are no less inventive, our goods and services no less needed than they were last week or last month or last year. Our capacity remains undiminished. But our time of standing pat, of protecting narrow interests and putting off unpleasant decisions -- that time has surely passed. Starting today, we must pick ourselves up, dust ourselves off, and begin again the work of remaking America.\n",
      "\n",
      "For everywhere we look, there is work to be done. The state of our economy calls for action, bold and swift, and we will act -- not only to create new jobs, but to lay a new foundation for growth. We will build the roads and bridges, the electric grids and digital lines that feed our commerce and bind us together. We will restore science to its rightful place, and wield technology's wonders to raise health care's quality and lower its cost. We will harness the sun and the winds and the soil to fuel our cars and run our factories. And we will transform our schools and colleges and universities to meet the demands of a new age. All this we can do. All this we will do.\n",
      "\n",
      "Now, there are some who question the scale of our ambitions -- who suggest that our system cannot tolerate too many big plans. Their memories are short. For they have forgotten what this country has already done; what free men and women can achieve when imagination is joined to common purpose, and necessity to courage.\n",
      "\n",
      "What the cynics fail to understand is that the ground has shifted beneath them -- that the stale political arguments that have consumed us for so long no longer apply. The question we ask today is not whether our government is too big or too small, but whether it works -- whether it helps families find jobs at a decent wage, care they can afford, a retirement that is dignified. Where the answer is yes, we intend to move forward. Where the answer is no, programs will end. And those of us who manage the public's dollars will be held to account -- to spend wisely, reform bad habits, and do our business in the light of day -- because only then can we restore the vital trust between a people and their government.\n",
      "\n",
      "Nor is the question before us whether the market is a force for good or ill. Its power to generate wealth and expand freedom is unmatched, but this crisis has reminded us that without a watchful eye, the market can spin out of control -- the nation cannot prosper long when it favors only the prosperous. The success of our economy has always depended not just on the size of our Gross Domestic Product, but on the reach of our prosperity; on the ability to extend opportunity to every willing heart -- not out of charity, but because it is the surest route to our common good.\n",
      "\n",
      "As for our common defense, we reject as false the choice between our safety and our ideals. Our Founding Fathers, faced with perils that we can scarcely imagine, drafted a charter to assure the rule of law and the rights of man, a charter expanded by the blood of generations. Those ideals still light the world, and we will not give them up for expedience's sake. And so to all the other peoples and governments who are watching today, from the grandest capitals to the small village where my father was born: know that America is a friend of each nation and every man, woman, and child who seeks a future of peace and dignity, and we are ready to lead once more.\n",
      "\n",
      "Recall that earlier generations faced down fascism and communism not just with missiles and tanks, but with the sturdy alliances and enduring convictions. They understood that our power alone cannot protect us, nor does it entitle us to do as we please. Instead, they knew that our power grows through its prudent use; our security emanates from the justness of our cause, the force of our example, the tempering qualities of humility and restraint.\n",
      "\n",
      "We are the keepers of this legacy. Guided by these principles once more, we can meet those new threats that demand even greater effort -- even greater cooperation and understanding between nations. We will begin to responsibly leave Iraq to its people, and forge a hard-earned peace in Afghanistan. With old friends and former foes, we will work tirelessly to lessen the nuclear threat, and roll back the specter of a warming planet. We will not apologize for our way of life, nor will we waver in its defense, and for those who seek to advance their aims by inducing terror and slaughtering innocents, we say to you now that our spirit is stronger and cannot be broken; you cannot outlast us, and we will defeat you.\n",
      "\n",
      "For we know that our patchwork heritage is a strength, not a weakness. We are a nation of Christians and Muslims, Jews and Hindus -- and non-believers. We are shaped by every language and culture, drawn from every end of this Earth; and because we have tasted the bitter swill of civil war and segregation, and emerged from that dark chapter stronger and more united, we cannot help but believe that the old hatreds shall someday pass; that the lines of tribe shall soon dissolve; that as the world grows smaller, our common humanity shall reveal itself; and that America must play its role in ushering in a new era of peace.\n",
      "\n",
      "To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect. To those leaders around the globe who seek to sow conflict, or blame their society's ills on the West -- know that your people will judge you on what you can build, not what you destroy. To those who cling to power through corruption and deceit and the silencing of dissent, know that you are on the wrong side of history; but that we will extend a hand if you are willing to unclench your fist.\n",
      "\n",
      "To the people of poor nations, we pledge to work alongside you to make your farms flourish and let clean waters flow; to nourish starved bodies and feed hungry minds. And to those nations like ours that enjoy relative plenty, we say we can no longer afford indifference to the suffering outside our borders; nor can we consume the world's resources without regard to effect. For the world has changed, and we must change with it.\n",
      "\n",
      "As we consider the road that unfolds before us, we remember with humble gratitude those brave Americans who, at this very hour, patrol far-off deserts and distant mountains. They have something to tell us, just as the fallen heroes who lie in Arlington whisper through the ages. We honor them not only because they are the guardians of our liberty, but because they embody the spirit of service; a willingness to find meaning in something greater than themselves. And yet, at this moment -- a moment that will define a generation -- it is precisely this spirit that must inhabit us all.\n",
      "\n",
      "For as much as government can do and must do, it is ultimately the faith and determination of the American people upon which this nation relies. It is the kindness to take in a stranger when the levees break, the selflessness of workers who would rather cut their hours than see a friend lose their job which sees us through our darkest hours. It is the firefighter's courage to storm a stairway filled with smoke, but also a parent's willingness to nurture a child, that finally decides our fate.\n",
      "\n",
      " Our challenges may be new. The instruments with which we meet them may be new. But those values upon which our success depends -- honesty and hard work, courage and fair play, tolerance and curiosity, loyalty and patriotism -- these things are old. These things are true. They have been the quiet force of progress throughout our history. What is demanded then is a return to these truths. What is required of us now is a new era of responsibility -- a recognition, on the part of every American, that we have duties to ourselves, our nation, and the world, duties that we do not grudgingly accept but rather seize gladly, firm in the knowledge that there is nothing so satisfying to the spirit, so defining of our character, than giving our all to a difficult task.\n",
      "\n",
      "This is the price and the promise of citizenship.\n",
      "\n",
      "This is the source of our confidence -- the knowledge that God calls on us to shape an uncertain destiny.\n",
      "\n",
      "This is the meaning of our liberty and our creed -- why men and women and children of every race and every faith can join in celebration across this magnificent mall, and why a man whose father less than sixty years ago might not have been served at a local restaurant can now stand before you to take a most sacred oath.\n",
      "\n",
      "So let us mark this day with remembrance, of who we are and how far we have traveled. In the year of America's birth, in the coldest of months, a small band of patriots huddled by dying campfires on the shores of an icy river. The capital was abandoned. The enemy was advancing. The snow was stained with blood. At a moment when the outcome of our revolution was most in doubt, the father of our nation ordered these words be read to the people:\n",
      "\n",
      "\"Let it be told to the future world ... that in the depth of winter, when nothing but hope and virtue could survive ... that the city and the country, alarmed at one common danger, came forth to meet ... it.\"\n",
      "\n",
      "America! In the face of our common dangers, in this winter of our hardship, let us remember these timeless words. With hope and virtue, let us brave once more the icy currents, and endure what storms may come. Let it be said by our children's children that when we were tested we refused to let this journey end, that we did not turn back nor did we falter; and with eyes fixed on the horizon and God's grace upon us, we carried forth that great gift of freedom and delivered it safely to future generations.\n",
      "\n",
      "Thank you. God bless you. And God bless the United States of America.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inaugural.raw('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'fellow', 'citizens', ':', 'I', 'stand', 'here', ...]\n"
     ]
    }
   ],
   "source": [
    "print(inaugural.words('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['My', 'fellow', 'citizens', ':'], ['I', 'stand', 'here', 'today', 'humbled', 'by', 'the', 'task', 'before', 'us', ',', 'grateful', 'for', 'the', 'trust', 'you', 'have', 'bestowed', ',', 'mindful', 'of', 'the', 'sacrifices', 'borne', 'by', 'our', 'ancestors', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(inaugural.sents('2009-Obama.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "* Find the total number of words (tokens) in Obama’s 2009 speech \n",
    "\n",
    "* Find the total number of distinct words (word types) in the same speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex1(doc_name):\n",
    "    # Use the plaintext corpus reader to access a pre-tokenised list of words\n",
    "    # for the document specified in \"doc_name\"\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "\n",
    "    # Find the total number of words in the speech\n",
    "    total_words = len(doc_words)\n",
    "\n",
    "    # Find the total number of DISTINCT words in the speech\n",
    "    total_distinct_words = len(set(w.lower() for w in doc_words))\n",
    "\n",
    "    # Return the word counts\n",
    "    return total_words, total_distinct_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in 2009-Obama.txt: 2726\n",
      "Total distinct words in 2009-Obama.txt: 900\n"
     ]
    }
   ],
   "source": [
    "speech_name = '2009-Obama.txt'\n",
    "(tokens,types) = ex1(speech_name)\n",
    "print('Total words in %s: %s' % (speech_name, tokens))\n",
    "print('Total distinct words in %s: %s' % (speech_name, types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Find the average word-type length of Obama’s 2009 speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex2(doc_name):\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "\n",
    "    # Construct a list that contains the word lengths for each DISTINCT word in the document\n",
    "    distinct_word_lengths = [len(w) for w in set(v.lower() for v in doc_words)]\n",
    "\n",
    "    # Find the average word type length\n",
    "    avg_word_length = float(sum(distinct_word_lengths)) / len(distinct_word_lengths)\n",
    "\n",
    "    # Return the average word type length of the document\n",
    "    return avg_word_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word type length for 2009-Obama.txt: 6.056\n"
     ]
    }
   ],
   "source": [
    "speech_name = '2009-Obama.txt'\n",
    "result2 = ex2(speech_name)\n",
    "print(\"Average word type length for %s: %.3f\"%(speech_name, result2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution\n",
    "\n",
    "A frequency distribution records the number of times each outcome of an ex-\n",
    "periment has occurred. For example, a frequency distribution could be used to\n",
    "record the number of times each word appears in a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the words from Barack Obama’s 2009 speech\n",
    "obama_words = inaugural.words('2009-Obama.txt')\n",
    "# Construct a frequency distribution over the lowercased words in the document\n",
    "fd_obama_words = nltk.FreqDist(w.lower() for w in obama_words)\n",
    "# Find the top 50 most frequently used words in the speech\n",
    "fd_obama_words.most_common(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily plot the top 50 words (note `%matplotlib inline` tells jupyter that it should embed plots in the output cell after you run the code. You only need to run it once per notebook, not in every cell with a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fd_obama_words.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out how many times the words peace and america were used in the speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('peace:', fd_obama_words['peace'])\n",
    "print('america:', fd_obama_words['america'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Compare the top 50 most frequent words in Barack Obama’s 2009 speech with\n",
    "George Washington’s 1789 speech.\n",
    "What can knowing word frequencies tell us about different speeches at different\n",
    "times in history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex3(doc_name, x):\n",
    "    doc_words = inaugural.words(doc_name)\n",
    "    \n",
    "    # Construct a frequency distribution over the lowercased words in the document\n",
    "    fd_doc_words = nltk.FreqDist(w.lower() for w in doc_words)\n",
    "\n",
    "    # Find the top x most frequently used words in the document\n",
    "    top_words = fd_doc_words.most_common(x)\n",
    "\n",
    "    # Return the top x most frequently used words\n",
    "    return top_words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now test your code\n",
    "print(\"Top 50 words for Obama's 2009 speech:\")\n",
    "result3a = ex3('2009-Obama.txt', 50)\n",
    "print(result3a)\n",
    "print(\"Top 50 words for Washington's 1789 speech:\")\n",
    "result3b = ex3('1789-Washington.txt', 50)\n",
    "print(result3b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "A statistical language model assigns a probability to a sequence of words, using\n",
    "a probability distribution. Language models have many applications in Natural\n",
    "Language Processing. For example, in speech recognition they may be used\n",
    "to predict the next word that a speaker will utter. In machine translation a\n",
    "language model may be used to score multiple candidate translations of an\n",
    "input sentence in order to find the most fluent/natural translation from the set\n",
    "of candidates.\n",
    "\n",
    "\n",
    "### Building a Language Model\n",
    "\n",
    "We provide you with an NgramModel module taken from an old version of NLTK. The\n",
    "initialisation method looks like this:\n",
    "\n",
    "    def __init__(self, n, train, pad_left=False, pad_right=False,\n",
    "    estimator=None, *estimator_args, **estimator_kwargs):\n",
    "\n",
    "Where:\n",
    "\n",
    "* n = order of the language model. 1=unigram; 2=bigram; 3=trigram, etc.\n",
    "* train = the training data (supplied as a list)\n",
    "* pad left and pad right = sentence initial and sentence final padding\n",
    "* estimator = method used to construct the probability distribution. May\n",
    "* or may not include smoothing. Arguments to the estimator are optional.\n",
    "\n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Use `NgramModel` to build a language model based on the text of Sense and\n",
    "Sensibility by Jane Austen. The language model should be a bigram model, and\n",
    "you can let it use the default `nltk.MLEProbDist` estimator.\n",
    "\n",
    "**Hint**, fill in the gaps with the information already provided in the code /\n",
    "comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK's NgramModel module (for building language models)\n",
    "# It has been removed from standard NLTK, so we access it from a local module\n",
    "try:\n",
    "    from nltk_model import *  # See the README inside the nltk_model folder for more information\n",
    "except ImportError:\n",
    "    from .nltk_model import *  # Compatibility depending on how this script was run\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: doc_name (string), n (int)\n",
    "# Output: lm (NgramModel language model)\n",
    "def ex4(doc_name, n):\n",
    "    # Construct a list of lowercase words from the document\n",
    "    words = [w.lower() for w in gutenberg.words(doc_name)]\n",
    "\n",
    "    # Build the language model using the nltk.MLEProbDist estimator \n",
    "    lm = NgramModel(n,words)\n",
    "    \n",
    "    # Return the language model (we'll use it in exercise 5)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your code for exercise 4\n",
    "result4 = ex4('austen-sense.txt', 2)\n",
    "print('Sense and Sensibility bigram language model built')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Probabilities\n",
    "\n",
    "Using the language model, we can work out the probability of a word given its\n",
    "context. In the case of the bigram language model built in Exercise 4, we have\n",
    "only one word of context. To obtain probabilities from a language model, use\n",
    "**NgramModel.prob**:\n",
    "\n",
    "`lm.prob(word, [context])`\n",
    "\n",
    "Where **word** and **context** are both unigram strings when working with a bigram\n",
    "language model. For higher order language models, context will be a list of\n",
    "unigram strings of length order-1.\n",
    "\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Using the bigram language model built in Exercise 4, compute the following\n",
    "probabilities\n",
    "\n",
    "1. reason followed by for\n",
    "2. the followed by end\n",
    "3. end followed by the\n",
    "\n",
    "Now uncomment the test code and check your results\n",
    "The result for c above is perhaps not what you expected. Why do you think it\n",
    "happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: lm (NgramModel language model, from exercise 4), word (string), context (list)\n",
    "# Output: p (float)\n",
    "def ex5(lm, word, context, verbose=False):\n",
    "    \n",
    "    # Compute the probability for the word given the context\n",
    "    p = lm.prob(word,context,verbose=verbose)\n",
    "\n",
    "    # Return the probability\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your code for Exercise 5\n",
    "result5a = ex5(result4, 'for', ['reason'])\n",
    "print(\"Probability of 'reason' followed by 'for': %.3f\" % result5a)\n",
    "result5b = ex5(result4, 'end', ['the'])\n",
    "print(\"Probability of 'the' followed by 'end': %.5f\" % result5b)\n",
    "result5c = ex5(result4, 'the', ['end'])\n",
    "print(\"Probability of 'end' followed by 'the': %.1f\" % result5c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Update your definition of the `ex5` function to include a (boolean) `verbose`\n",
    "argument, which is passed through to `NgramModel.prob`. Use this to see if\n",
    "it gives any insight on the (end, the) bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your code for Exercise 6\n",
    "result6 = ex5(result4, 'the', ['end'], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Further\n",
    "### Smoothing\n",
    "\n",
    "Try using an estimator which does do smoothing, and see what happens to all\n",
    "three of the bigram probabilities. Try `help(NgramModel)` for help with the operation of this class and how to supply estimators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see what estimators are available run:\n",
    "from nltk import probability\n",
    "help(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You will need to edit your existing definition of the `ex4` function to pass in an estimator function.  You can look at the `_estimator` function in the `nltk_model/ngram.py` file for an example of the kind of argument required.\n",
    "\n",
    "You can use the code block below for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tokenisation Padding\n",
    "\n",
    "So far you’ve treated the data as a flat list of ‘words’, which doesn’t fully address\n",
    "the place of words within sentences. Using `gutenberg.sents(...)` explore the\n",
    "impact of the `pad left` and `pad right` argument to `NgramModel` by further editting `ex4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Save your new model as `lm` and then compare the\n",
    "following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.prob('The', ['<s>']))\n",
    "print(lm.prob('the', ['<s>']))\n",
    "print(lm.prob('End', ['<s/>']))\n",
    "print(lm.prob('end', ['<s/>']))\n",
    "print(lm.prob('.', ['<s/>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs vs. probabilities\n",
    "\n",
    "Redo the previous two sub-sections using *costs* instead of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lm.logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
